{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7193aee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction.shape torch.Size([1, 8400, 85])\n",
      "prediction[:, :, 2] : torch.Size([1, 8400]) prediction[:, :, 0] torch.Size([1, 8400]), box_corner[:, :, 0] : torch.Size([1, 8400])\n",
      "conf_mask : tensor([False, False, False,  ..., False, False, False], device='cuda:0'), torch.Size([8400])\n",
      "image_pred[:, :5] : torch.Size([8400, 5])\n",
      "detections: tensor([[-3.8707e+00,  1.8199e-01,  4.2044e+01,  ...,  8.0466e-06,\n",
      "          1.0606e-01,  0.0000e+00],\n",
      "        [-1.3385e+01,  6.0095e-02,  5.7748e+01,  ...,  2.4736e-06,\n",
      "          8.5399e-02,  0.0000e+00],\n",
      "        [-7.1481e+00,  6.5004e-01,  7.5329e+01,  ...,  1.6987e-06,\n",
      "          6.1109e-02,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 3.0487e+02,  4.0363e+02,  8.1234e+02,  ...,  3.9488e-05,\n",
      "          5.4466e-02,  0.0000e+00],\n",
      "        [ 3.6552e+02,  3.9675e+02,  7.9137e+02,  ...,  1.5587e-05,\n",
      "          1.1562e-01,  0.0000e+00],\n",
      "        [ 4.6104e+02,  4.2508e+02,  7.2706e+02,  ...,  1.4603e-05,\n",
      "          1.0546e-01,  0.0000e+00]], device='cuda:0')\n",
      "detections[:, :4] : tensor([[114.8802,  48.5070, 257.8797, 423.4343],\n",
      "        [115.7449,  47.0075, 259.7402, 424.8922],\n",
      "        [115.9920,  47.1280, 260.5959, 424.8651],\n",
      "        [223.4444,  32.9690, 406.5351, 424.2924],\n",
      "        [222.8453,  36.1822, 405.9692, 423.4330],\n",
      "        [222.8308,  31.7812, 406.5620, 425.1997],\n",
      "        [390.0880,  45.3419, 545.7521, 425.8274],\n",
      "        [391.3277,  46.9900, 545.6981, 426.3704],\n",
      "        [390.3478,  48.9104, 546.7054, 423.6262],\n",
      "        [117.6253,  45.5450, 258.4174, 424.5634],\n",
      "        [116.0641,  45.5635, 258.5456, 426.6772],\n",
      "        [116.7024,  46.9272, 258.3913, 427.0164],\n",
      "        [115.5544,  47.2145, 256.0084, 424.9189],\n",
      "        [223.0484,  33.7869, 406.7463, 423.9400],\n",
      "        [222.2502,  33.7158, 406.0398, 425.0392],\n",
      "        [223.6385,  37.3889, 406.7542, 423.1799],\n",
      "        [391.4449,  45.8060, 546.4327, 426.1924],\n",
      "        [391.2008,  46.3932, 546.7872, 426.5155],\n",
      "        [391.0999,  47.7593, 546.0174, 425.8245],\n",
      "        [115.5384,  48.3937, 259.3052, 425.9014],\n",
      "        [116.8196,  46.5201, 258.4250, 426.2796],\n",
      "        [221.7053,  37.7726, 407.1612, 424.7882],\n",
      "        [221.3160,  37.8446, 407.5136, 424.9273],\n",
      "        [223.1966,  39.4460, 405.9471, 421.9518],\n",
      "        [391.2843,  47.2527, 547.2877, 426.5508],\n",
      "        [391.4755,  46.1460, 546.5125, 426.1033],\n",
      "        [390.8712,  47.6458, 545.6763, 424.1224]], device='cuda:0'), torch.Size([27, 4])\n",
      "detections[:, 4] * detections[:, 5] : tensor([0.7430, 0.8202, 0.8331, 0.8958, 0.8933, 0.7453, 0.8693, 0.8905, 0.8900,\n",
      "        0.8025, 0.8258, 0.8339, 0.6816, 0.8935, 0.8963, 0.8787, 0.8909, 0.8937,\n",
      "        0.8951, 0.8220, 0.8422, 0.8935, 0.8947, 0.5792, 0.8929, 0.8936, 0.8319],\n",
      "       device='cuda:0')\n",
      "detections[:, 6] : tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0.], device='cuda:0'), torch.Size([27])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TensorRT] WARNING: TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.2.0\n",
      "[TensorRT] WARNING: TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.2.0\n"
     ]
    }
   ],
   "source": [
    "## detect.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "import cv2\n",
    "import torchvision\n",
    "import torch\n",
    "import time\n",
    "import traceback\n",
    "import importlib\n",
    "import common\n",
    "\n",
    "\n",
    "class DETECTOR:\n",
    "\n",
    "    def __init__(self):      \n",
    "\n",
    "        \n",
    "        self.num_classes = 80\n",
    "        self.confthre = 0.5\n",
    "        self.nmsthre = 0.3\n",
    "\n",
    "        \n",
    "        self.input_h = 640\n",
    "        self.input_w = 640\n",
    "        self.device = torch.device('cuda:0')\n",
    "        \n",
    "        \n",
    "        self.categories = {} \n",
    "        # Sports\n",
    "        self.categories['1'] = [35,36,30,37,31,32,33,38,39] \n",
    "        # Vehicle\n",
    "        self.categories['2'] = [5,2,9,6,3,4,7,8]\n",
    "        # Food\n",
    "        self.categories['3'] = [48,47,51,56,52,55,53,50,54,49]\n",
    "        # Product\n",
    "        self.categories['4'] = [60,74,40,46,68,57,75,42,61,43,79,67,34,44,64,69,70,59,73,66,77,72,58,45,78,71,80,63,76,41]\n",
    "        # Animal\n",
    "        self.categories['5'] = [22,15,16,20,17,21,24,18,65,1,19,23]\n",
    "        # Structure\n",
    "        self.categories['6'] = [14,11,13,12,62,10]\n",
    "        # Accessory\n",
    "        self.categories['7'] = [25,27,29,28,26]\n",
    "        # person\n",
    "        self.categories['8'] = [1]\n",
    "        \n",
    "        \n",
    "    \n",
    "    def load_set_meta(self, channel_id=None, model_parameter=None, channel_info_dict=None, model_name=None):\n",
    "\n",
    "        category_id = model_parameter['category_id']\n",
    "        channel_info_dict[channel_id]['map_data'][model_name]['category_id'] = category_id\n",
    "\n",
    "    \n",
    "    def load(self, weights):\n",
    "        self.weights = weights\n",
    "        self.Engine = common.Engine()      \n",
    "        self.Engine.make_context(self.weights)\n",
    "        self.batchsize = int(self.Engine.input_shape[0])\n",
    "\n",
    "        \n",
    "#         self.init_sample_data()\n",
    "        \n",
    "        \n",
    "    def init_sample_data(self):\n",
    "        try:\n",
    "            base_path = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))\n",
    "            sample_path = os.path.join(base_path, 'sample.png')\n",
    "            sample_img = cv2.imread(sample_path)\n",
    "\n",
    "            sample_img_batch = [sample_img] * self.batch_size\n",
    "            sample_img_batch = torch.tensor(sample_img_batch).cuda()\n",
    "            \n",
    "            self.inference_batch(sample_img_batch)\n",
    "        except Exception as e:\n",
    "            print(f'initialize sample image : {e}')\n",
    "            print(traceback.format_exc())\n",
    "        \n",
    "\n",
    "    def parse_input(self,input_data_batch):\n",
    "        res = []\n",
    "        for input_data in input_data_batch:\n",
    "            frame = input_data['framedata']['frame']\n",
    "            bbox = input_data['bbox']\n",
    "            cropped_img = common.getCropByFrame(frame,bbox)\n",
    "            res.append(cropped_img)\n",
    "        return res\n",
    "       \n",
    "\n",
    "    \n",
    "    \n",
    "    def preprocess(self,frame_batch) :  \n",
    "        result = torch.zeros([len(frame_batch), 3, self.input_h, self.input_w], dtype=torch.float32, device=torch.device(\"cuda:0\")).fill_(144)\n",
    "        scale_list = []\n",
    "        for idx, frame in enumerate(frame_batch) :\n",
    "\n",
    "#             frame = frame.to(torch.device(\"cuda:0\"))\n",
    "            _, h, w = frame.shape\n",
    "            \n",
    "            r = min(self.input_h/h, self.input_w/w)\n",
    "            if r < 1 :  \n",
    "                rw, rh = int(r*w), int(r*h)\n",
    "\n",
    "                resized_img = torchvision.transforms.functional.resize(frame, (rh,rw)).float()\n",
    "                result[idx, :,:rh,:rw] = resized_img \n",
    "                scale_list.append(r)\n",
    "            else : \n",
    "\n",
    "                result[idx, :,:h,:w] = frame\n",
    "                scale_list.append(None)\n",
    "\n",
    "            \n",
    "        return result, scale_list\n",
    "\n",
    "    \n",
    "    def preprocess_for_calibrator(self,frame_batch) :  \n",
    "        result = torch.zeros([len(frame_batch), 3, self.input_h, self.input_w], dtype=torch.float32, device=torch.device(\"cpu\")).fill_(144)\n",
    "        scale_list = []\n",
    "        for idx, frame in enumerate(frame_batch) :\n",
    "\n",
    "#             frame = frame.to(torch.device(\"cuda:0\"))\n",
    "            _, h, w = frame.shape\n",
    "            \n",
    "            r = min(self.input_h/h, self.input_w/w)\n",
    "            if r < 1 :  \n",
    "                rw, rh = int(r*w), int(r*h)\n",
    "\n",
    "                resized_img = torchvision.transforms.functional.resize(frame, (rh,rw)).float()\n",
    "                result[idx, :,:rh,:rw] = resized_img \n",
    "                scale_list.append(r)\n",
    "            else : \n",
    "\n",
    "                result[idx, :,:h,:w] = frame\n",
    "                scale_list.append(None)\n",
    "\n",
    "            \n",
    "        return result, scale_list\n",
    "\n",
    "\n",
    "\n",
    "    def inference(self,input_data) : \n",
    "        output_data = self.Engine.do_inference_v2(input_data)\n",
    "        return output_data[0]\n",
    "\n",
    "\n",
    "    def postprocess(self,prediction,scale_list, class_agnostic=False):\n",
    "        print(f'prediction.shape {prediction.shape}')\n",
    "        box_corner = prediction.new(prediction.shape)\n",
    "\n",
    "        box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2 # torch.Size([1, 8400]\n",
    "        box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2\n",
    "        box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2\n",
    "        box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2\n",
    "        prediction[:, :, :4] = box_corner[:, :, :4] #torch.Size([1, 8400, 4])\n",
    "#         print(f'box_corner[:, :, 0]  : {box_corner[:, :, 0],box_corner[:, :, 1] ,box_corner[:, :, 2] ,box_corner[:, :, 3] }')\n",
    "#         print(f'box_corner[:, :, 0]  : {box_corner[:, :, 0].shape,box_corner[:, :, 1] ,box_corner[:, :, 2] ,box_corner[:, :, 3] }')\n",
    "            \n",
    "#         print(f'box_corner[:, :, :4] : {box_corner[:, :, :4]} {box_corner[:, :, :4].shape}')\n",
    "        print(f'prediction[:, :, 2] : {prediction[:, :, 2].shape} prediction[:, :, 0] {prediction[:, :, 0].shape}, box_corner[:, :, 0] : {box_corner[:, :, 0].shape}')\n",
    "        #prediction[:, :, 2] : torch.Size([1, 8400]) prediction[:, :, 0] torch.Size([1, 8400]), box_corner[:, :, 0] : torch.Size([1, 8400])\n",
    "        outputs = list()\n",
    "        #prediction 이미지 개수만큼 for문 돌기\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i, image_pred in enumerate(prediction):\n",
    "#             print(f'image_pred : {image_pred}')\n",
    "#             print(f'image_pred.shape : {image_pred.shape}') #image_pred.shape : torch.Size([8400, 85])\n",
    "            if not image_pred.size(0):\n",
    "                outputs.append(None)\n",
    "                continue\n",
    "            #print(f'image_pred[:, 5: 5 + self.num_classes] : {image_pred[:, 5: 5 + self.num_classes].shape}')\n",
    "            #image_pred[:, 5: 5 + self.num_classes] : torch.Size([8400, 80])\n",
    "            # score, 라벨\n",
    "            class_conf, class_pred = torch.max(image_pred[:, 5: 5 + self.num_classes], 1, keepdim=True)\n",
    "#             print(f'class_conf : {class_conf}, class_pred : {class_pred} ')\n",
    "#             print(f'class_conf : {class_conf.shape} ,class_pred : {class_pred.shape} ')\n",
    "#             class_conf : torch.Size([8400, 1]) ,class_pred : torch.Size([8400, 1]) \n",
    "            \n",
    "            conf_mask = (image_pred[:, 4] * class_conf.squeeze() >= self.confthre).squeeze()\n",
    "            print(f'conf_mask : {conf_mask}, {conf_mask.shape}') #torch.Size([8400])\n",
    "            print(f'image_pred[:, :5] : {image_pred[:, :5].shape}')\n",
    "            detections = torch.cat((image_pred[:, :5], class_conf, class_pred.float()), 1)\n",
    "            print(f'detections: {detections}')\n",
    "            \n",
    "#             image_pred[:, :5] : torch.Size([8400, 5])\n",
    "#             detections: torch.Size([8400, 7])\n",
    "            \n",
    "            detections = detections[conf_mask]\n",
    "            if not detections.size(0):\n",
    "                outputs.append(None)\n",
    "                continue\n",
    "            print(f'detections[:, :4] : {detections[:, :4]}, {detections[:, :4].shape}')\n",
    "            print(f'detections[:, 4] * detections[:, 5] : {detections[:, 4] * detections[:, 5]}')\n",
    "            print(f'detections[:, 6] : {detections[:, 6]}, {detections[:, 6].shape}')\n",
    "            if class_agnostic:\n",
    "                nms_out_index = torchvision.ops.nms(\n",
    "                    detections[:, :4],\n",
    "                    detections[:, 4] * detections[:, 5],\n",
    "                    self.nmsthre,\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                nms_out_index = torchvision.ops.batched_nms(\n",
    "                    detections[:, :4],\n",
    "                    detections[:, 4] * detections[:, 5],\n",
    "                    detections[:, 6],\n",
    "                    self.nmsthre,\n",
    "                )\n",
    "\n",
    "            detections = detections[nms_out_index]\n",
    "            \n",
    "\n",
    "            s = time.time()\n",
    "            ### bbox 복원\n",
    "            output = list()\n",
    "            for j, det in enumerate(detections):\n",
    "                if isinstance(det, type(None)):\n",
    "                    output.append(None)\n",
    "                    continue\n",
    "\n",
    "                tmp = det.detach().cpu().numpy()\n",
    "                label = str(int(tmp[6]))\n",
    "                    \n",
    "                x1, y1, x2, y2 = map(int, tmp[:4])\n",
    "                t2 = time.time()\n",
    "\n",
    "                restore_bbox = common.restoreBboxScale( [x1,y1,x2,y2], (scale_list[i],scale_list[i]) )\n",
    "                t3 = time.time()\n",
    "\n",
    "                tmp[0] = restore_bbox[0]\n",
    "                tmp[1] = restore_bbox[1]\n",
    "                tmp[2] = restore_bbox[2]\n",
    "                tmp[3] = restore_bbox[3]\n",
    "                \n",
    "                                \n",
    "                score1 = tmp[4]\n",
    "                score2 = tmp[5]\n",
    "                score = (tmp[4] + tmp[5]) / 2\n",
    "                \n",
    "                x1, y1, x2, y2 = map(int, restore_bbox)            \n",
    "                bbox = [x1, y1, x2, y2]      \n",
    "                output.append({\"bbox\":bbox, \"score\":score, \"label\":label})\n",
    "                t4 = time.time()\n",
    "            outputs.append(output)\n",
    "            e = time.time()\n",
    "           \n",
    "        return outputs             \n",
    "  \n",
    "\n",
    "    \n",
    "weights = '/DATA_17/trt_test/engines/yoloxm_test_ij/yoloxm_int8_024.trt'\n",
    "\n",
    "img_path = '/DATA_17/ij/test/test_image.jpeg'\n",
    "# image = cv2.imread(img_path)[..., ::-1]#BGR 순서를 RGB로 뒤집습니다.\n",
    "image = cv2.imread(img_path)\n",
    "image = np.copy(image)\n",
    "image = torch.from_numpy(image).to(torch.device(\"cuda\"))\n",
    "image = image.permute(2,0,1)\n",
    "frame_batch = [image]\n",
    "\n",
    "\n",
    "\n",
    "pe = DETECTOR()\n",
    "pe.load(weights)\n",
    "\n",
    "\n",
    "input_data, scale_list = pe.preprocess(frame_batch)\n",
    "\n",
    "result = pe.inference(input_data)\n",
    "\n",
    "\n",
    "output_batch = pe.postprocess(result, scale_list)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dffe3963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.8707e+00,  1.8199e-01,  4.2044e+01,  ...,  2.1921e-03,\n",
       "          1.5117e-03,  2.3325e-02],\n",
       "        [-1.3385e+01,  6.0095e-02,  5.7748e+01,  ...,  1.6623e-03,\n",
       "          1.2262e-03,  2.1149e-02],\n",
       "        [-7.1481e+00,  6.5004e-01,  7.5329e+01,  ...,  1.1270e-03,\n",
       "          1.2085e-03,  2.0137e-02],\n",
       "        ...,\n",
       "        [ 3.0487e+02,  4.0363e+02,  8.1234e+02,  ...,  5.5450e-03,\n",
       "          3.5926e-03,  7.2937e-03],\n",
       "        [ 3.6552e+02,  3.9675e+02,  7.9137e+02,  ...,  5.0956e-03,\n",
       "          4.7047e-03,  7.9430e-03],\n",
       "        [ 4.6104e+02,  4.2508e+02,  7.2706e+02,  ...,  5.9679e-03,\n",
       "          4.9112e-03,  9.6800e-03]], device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf48c04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
