{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2b6f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "# from interface_dynamic import PEOPLE_DETECTOR\n",
    "from interface_people import PEOPLE_DETECTOR\n",
    "import time\n",
    "torch.cuda.init()\n",
    "\n",
    "def proc(video_path):\n",
    "    frame_list = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)  # float\n",
    "    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)  # float\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    cnt = 0\n",
    "    if cap.isOpened() :    \n",
    "        while True:\n",
    "            cnt+=1\n",
    "            ret, frame = cap.read()\n",
    "            if ret == False :\n",
    "                break\n",
    "            if cnt < 0:\n",
    "                continue\n",
    "            \n",
    "\n",
    "\n",
    "    return frame_list\n",
    "                \n",
    "video_path = '/DATA_17/ij/peopleNet_test/0001_compressed.mp4'\n",
    "weights = '/DATA_17/ij/peopleNet_test/best_model_people.trt'\n",
    "\n",
    "\n",
    "\n",
    "frame_list = proc(video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b9528af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TensorRT] WARNING: Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.\n",
      "[TensorRT] WARNING: TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.2.0\n",
      "[TensorRT] WARNING: TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame load done.\n",
      "end : 4.0210466384887695\n",
      "end : 4.064268112182617\n",
      "end : 4.0764124393463135\n",
      "end : 4.099853515625\n",
      "end : 4.124309062957764\n",
      "end : 4.136652231216431\n",
      "end : 4.15329122543335\n",
      "end : 4.159082651138306\n",
      "end : 4.1699981689453125\n",
      "end : 4.177017688751221\n"
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# import torch\n",
    "# # from interface_dynamic import PEOPLE_DETECTOR\n",
    "# from interface_people import PEOPLE_DETECTOR\n",
    "# import time\n",
    "# torch.cuda.init()\n",
    "\n",
    "# def proc(video_path):\n",
    "#     frame_list = []\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)  # float\n",
    "#     height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)  # float\n",
    "#     fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "#     cnt = 0\n",
    "#     if cap.isOpened() :    \n",
    "#         while True:\n",
    "#             cnt+=1\n",
    "#             ret, frame = cap.read()\n",
    "#             if ret == False :\n",
    "#                 break\n",
    "#             if cnt < 0:\n",
    "#                 continue\n",
    "            \n",
    "#             infe_frame = torch.from_numpy(frame).to(torch.device(\"cuda\"),non_blocking=True)\n",
    "#             infe_frame = infe_frame.permute(2, 0, 1) \n",
    "#             input_data = dict()\n",
    "#             input_data[\"framedata\"] = {\"frame\":infe_frame}\n",
    "#             input_data[\"framedata\"]['meta'] = {'source' : {'channel_id' : str(1_10), 'frame_count' : cnt }}\n",
    "#             input_data[\"bbox\"] = [0,0,infe_frame.shape[2],infe_frame.shape[1]]\n",
    "#             input_data[\"scenario\"] = \"s\"   \n",
    "#             input_data[\"data\"] = None   \n",
    "#             frame_list.append(input_data)\n",
    "#     return frame_list\n",
    "                \n",
    "# video_path = '/DATA_17/ij/peopleNet_test/0001_compressed.mp4'\n",
    "# weights = '/DATA_17/ij/peopleNet_test/best_model_people.trt'\n",
    "\n",
    "# pe = PEOPLE_DETECTOR()\n",
    "# pe.load(weights)\n",
    "\n",
    "# frame_list = proc(video_path)\n",
    "# print('frame load done.')\n",
    "# for i in range(10) : \n",
    "#     start = time.time()\n",
    "#     for input_data in frame_list : \n",
    "#         x, scale_list = pe.run_inference([input_data],[])\n",
    "\n",
    "#     print(f'end : {time.time() -start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbf4ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import torch\n",
    "# from interface_people import PEOPLE_DETECTOR\n",
    "# import time\n",
    "# torch.cuda.init()\n",
    "# img_path = '/DATA_17/ij/peopleNet_test/test_image.jpeg'\n",
    "# image = cv2.imread(img_path)\n",
    "# weights = '/DATA_17/ij/peopleNet_test/best_model_people.trt'\n",
    "\n",
    "# infe_frame = torch.from_numpy(image).to(torch.device(\"cuda\"))\n",
    "# infe_frame = infe_frame.permute(2, 0, 1) \n",
    "# input_data = dict()\n",
    "# input_data[\"framedata\"] = {\"frame\":infe_frame}\n",
    "# input_data[\"framedata\"]['meta'] = {'source' : {'channel_id' : str(1_10), 'frame_count' : 0 }}\n",
    "# input_data[\"bbox\"] = [0,0,infe_frame.shape[2],infe_frame.shape[1]]\n",
    "# input_data[\"scenario\"] = \"s\"   \n",
    "# input_data[\"data\"] = None   \n",
    "# pe = PEOPLE_DETECTOR()\n",
    "# pe.load(weights)\n",
    "# x, scale_list = pe.run_inference([input_data],[])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c13f801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import time\n",
    "# import torchvision\n",
    "\n",
    "# norm_data_x = pe.norm_data_x\n",
    "\n",
    "# norm_data_y = pe.norm_data_y\n",
    "# label_tensor = pe.label_tensor\n",
    "# nmsthre = pe.nmsthre\n",
    "# input_w = pe.input_w\n",
    "# input_h = pe.input_h\n",
    "# threshold = pe.threshold\n",
    "\n",
    "# @torch.jit.script\n",
    "# def postprocess(norm_data_x, norm_data_y, label_tensor, tensor_bbox,tensor_score):\n",
    "\n",
    "#     tensor_x1 = torch.cat([tensor_bbox[:2040],tensor_bbox[8160:10200],tensor_bbox[16320:18360]],dim=0)\n",
    "#     tensor_y1 = torch.cat([tensor_bbox[2040:4080],tensor_bbox[10200:12240],tensor_bbox[18360:20400]],dim=0)\n",
    "#     tensor_x2 = torch.cat([tensor_bbox[4080:6120],tensor_bbox[12240:14280],tensor_bbox[20400:22440]],dim=0)\n",
    "#     tensor_y2 = torch.cat([tensor_bbox[6120:8160],tensor_bbox[14280:16320],tensor_bbox[22440:]],dim=0)\n",
    "\n",
    "#     tensor_x1 = (tensor_x1 - norm_data_x) * -35\n",
    "#     tensor_y1 = (tensor_y1 - norm_data_y) * -35\n",
    "#     tensor_x2 = (tensor_x2 + norm_data_x) * 35\n",
    "#     tensor_y2 = (tensor_y2 + norm_data_y) * 35\n",
    "\n",
    "#     total_bbox_tensor = torch.stack([tensor_x1,tensor_y1,tensor_x2,tensor_y2],dim=1)\n",
    "#     score_tensor_mask = tensor_score >= 0.7\n",
    "\n",
    "#     t_box = total_bbox_tensor[score_tensor_mask]\n",
    "#     t_score = tensor_score[score_tensor_mask]\n",
    "#     t_label = label_tensor[score_tensor_mask]\n",
    "\n",
    "\n",
    "#     return t_box,t_score,t_label\n",
    "\n",
    "\n",
    "\n",
    "# # start = torch.cuda.Event(enable_timing=True)\n",
    "# # end = torch.cuda.Event(enable_timing=True)\n",
    "# # start.record()  \n",
    "# s = time.time()\n",
    "# for i in range(10000):\n",
    "#     t_box,t_score,t_label = postprocess(norm_data_x, norm_data_y, label_tensor, x[0],x[1])\n",
    "# print(time.time()-s)\n",
    "# # end.record()\n",
    "# # torch.cuda.synchronize()\n",
    "# # print(f\"infer time : {start.elapsed_time(end)}\")\n",
    "\n",
    "\n",
    "\n",
    "# # ori_h = 426\n",
    "# # ori_w = 640\n",
    "    \n",
    "# nms_out_index = torchvision.ops.batched_nms(\n",
    "#     t_box,\n",
    "#     t_score,\n",
    "#     t_label,\n",
    "#     0.5,\n",
    "# )\n",
    "\n",
    "# ############################################\n",
    "# result_box = t_box[nms_out_index].detach().cpu().numpy()\n",
    "# result_score = t_score[nms_out_index].detach().cpu().numpy()\n",
    "# result_label = t_label[nms_out_index].detach().cpu().numpy()\n",
    "# output_list = []\n",
    "# for idx, bbox in enumerate(result_box):\n",
    "#     if isinstance(bbox, type(None)):\n",
    "#         output_list.append(None)\n",
    "#         continue\n",
    "#     x1, y1, x2, y2 = map(int, bbox)\n",
    "#     x1 = int((x1 * ori_w)/960)\n",
    "#     y1 = int((y1 * ori_h)/544)\n",
    "#     x2 = int((x2 * ori_w)/960)\n",
    "#     y2 = int((y2 * ori_h)/544)\n",
    "#     score = str(result_score[idx])\n",
    "#     label = int(result_label[idx])\n",
    "#     out = {\"bbox\":[x1, y1, x2, y2], \"score\":score, \"label\":label}\n",
    "#     output_list.append(out)\n",
    "# ############################################\n",
    "\n",
    "# # print(output_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1c844c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.00023124890327453612, 0.00017664525508880616)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 일반 \n",
    "# infer time : 2417.224853515625\n",
    "# 2.3124890327453613\n",
    "\n",
    "# 스크립트\n",
    "# infer time : 1750.208984375\n",
    "# 1.7664525508880615\n",
    "\n",
    "\n",
    "# 1.3배 정도 느리다 \n",
    "non = 2.3124890327453613/10000\n",
    "script = 1.7664525508880615/10000\n",
    "non, script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f2d144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
