{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e57e330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TensorRT] WARNING: The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. TensorRT maintains only a single logger pointer at any given time, so the existing value, which can be retrieved with getLogger(), will be used instead. In order to use a new logger, first destroy all existing builder, runner or refitter objects.\n",
      "\n",
      "[TensorRT] WARNING: Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.\n",
      "[TensorRT] WARNING: TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.2.0\n",
      "[TensorRT] WARNING: TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.2.0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "import cv2\n",
    "import torchvision\n",
    "import torch\n",
    "import time\n",
    "import traceback\n",
    "import importlib\n",
    "import common_dynamic as common\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "torch.cuda.init()\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "import cv2\n",
    "import torchvision\n",
    "import torch\n",
    "import time\n",
    "import traceback\n",
    "import importlib\n",
    "import common_dynamic as common\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# torch.cuda.init()\n",
    "\n",
    "\n",
    "class PEOPLE_DETECTOR:\n",
    "\n",
    "    def __init__(self):  \n",
    "#         self.logger = logger\n",
    "\n",
    "        \n",
    "        self.num_classes = list(range(3))\n",
    "        self.threshold = 0.7\n",
    "        self.nmsthre = 0.5\n",
    "\n",
    "\n",
    "        self.input_h = 544\n",
    "        self.input_w = 960\n",
    "\n",
    "        self.stride = 16\n",
    "        self.box_norm = 35.0\n",
    "        self.grid_h = 34\n",
    "        self.grid_w = 60\n",
    "        \n",
    "        self.grid_calculator()\n",
    "\n",
    "    def load(self, weights):\n",
    "        self.weights = weights\n",
    "        self.Engine = common.Engine()      \n",
    "        self.Engine.make_context(self.weights)\n",
    "        self.batchsize = 1\n",
    "\n",
    "    def parse_input(self,input_data_batch):\n",
    "        res = []\n",
    "        for input_data in input_data_batch:\n",
    "            frame = input_data['framedata']['frame']\n",
    "            bbox = input_data['bbox']\n",
    "            cropped_img = common.getCropByFrame(frame,bbox)\n",
    "            res.append(cropped_img)\n",
    "        return res\n",
    "    \n",
    "    \n",
    "    def grid_calculator(self):\n",
    "\n",
    "        norm_data_x = torch.zeros([self.grid_h, self.grid_w], dtype=torch.float32, device=torch.device(\"cuda\"))\n",
    "        norm_data_y = torch.zeros([self.grid_h, self.grid_w], dtype=torch.float32, device=torch.device(\"cuda\"))\n",
    "\n",
    "        self.grid_h = int(self.input_h / self.stride)\n",
    "        self.grid_w = int(self.input_w / self.stride)\n",
    "        self.grid_size = self.grid_h * self.grid_w\n",
    "\n",
    "        for i in range(self.grid_h):\n",
    "            value = (i * self.stride + 0.5) / self.box_norm\n",
    "            norm_data_y[i,:] = value\n",
    "\n",
    "        for i in range(self.grid_w):\n",
    "            value = (i * self.stride + 0.5) / self.box_norm\n",
    "            norm_data_x[:,i] = value\n",
    "#         self.norm_data_x = norm_data_x\n",
    "#         self.norm_data_y = norm_data_y    \n",
    "        self.norm_data_x = torch.stack((norm_data_x,norm_data_x,norm_data_x))\n",
    "        self.norm_data_y = torch.stack((norm_data_y,norm_data_y,norm_data_y))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def postprocess(self,outputs, scale_list):\n",
    "        \"\"\"\n",
    "        Postprocesses the inference output\n",
    "        Args:\n",
    "            outputs (list of float): inference output\n",
    "            min_confidence (float): min confidence to accept detection\n",
    "            analysis_classes (list of int): indices of the classes to consider\n",
    "\n",
    "        Returns: list of list tuple: each element is a two list tuple (x, y) representing the corners of a bb\n",
    "        \"\"\"\n",
    "        output_list = []\n",
    "        ori_h = scale_list[0][0]\n",
    "        ori_w = scale_list[0][1]\n",
    "\n",
    "        tensor_x1 = torch.stack((outputs[0][0],outputs[0][4],outputs[0][8]))\n",
    "        tensor_y1 = torch.stack((outputs[0][1],outputs[0][5],outputs[0][9]))\n",
    "        tensor_x2 = torch.stack((outputs[0][2],outputs[0][6],outputs[0][10]))\n",
    "        tensor_y2 = torch.stack((outputs[0][3],outputs[0][7],outputs[0][11]))\n",
    "\n",
    "#         tensor_x1 = torch.cat([outputs[0][0],outputs[0][4],outputs[0][8]],0).view(-1,self.grid_h,self.grid_w)\n",
    "#         tensor_y1 = torch.cat([outputs[0][1],outputs[0][5],outputs[0][9]],0).view(-1,self.grid_h,self.grid_w)\n",
    "#         tensor_x2 = torch.cat([outputs[0][2],outputs[0][6],outputs[0][10]],0).view(-1,self.grid_h,self.grid_w)\n",
    "#         tensor_y2 = torch.cat([outputs[0][3],outputs[0][7],outputs[0][11]],0).view(-1,self.grid_h,self.grid_w)\n",
    "        tensor_x1 = (tensor_x1 - self.norm_data_x) * -35\n",
    "        tensor_y1 = (tensor_y1 - self.norm_data_y) * -35\n",
    "        tensor_x2 = (tensor_x2 + self.norm_data_x) * 35\n",
    "        tensor_y2 = (tensor_y2 + self.norm_data_y) * 35\n",
    "\n",
    "        \n",
    "        score_tensor = outputs[1] >= self.threshold\n",
    "        \n",
    "        for label in self.num_classes : \n",
    "#             ii = label * 4\n",
    "            \n",
    "#             tensor_x1 = (outputs[0][ii] - self.norm_data_x) * -35\n",
    "#             tensor_y1 = (outputs[0][ii + 1] - self.norm_data_y) * -35\n",
    "#             tensor_x2 = (outputs[0][ii + 2] + self.norm_data_x) * 35\n",
    "#             tensor_y2 = (outputs[0][ii + 3] + self.norm_data_y) * 35\n",
    "\n",
    "#             new_x1 = tensor_x1[score_tensor[label]]\n",
    "#             new_y1 = tensor_y1[score_tensor[label]]\n",
    "#             new_x2 = tensor_x2[score_tensor[label]]\n",
    "#             new_y2 = tensor_y2[score_tensor[label]]\n",
    "\n",
    "            new_x1 = tensor_x1[label][score_tensor[label]]\n",
    "            new_y1 = tensor_y1[label][score_tensor[label]]\n",
    "            new_x2 = tensor_x2[label][score_tensor[label]]\n",
    "            new_y2 = tensor_y2[label][score_tensor[label]]\n",
    "\n",
    "\n",
    "            box = torch.stack((new_x1,new_y1,new_x2,new_y2),1)\n",
    "            score = outputs[1][label][score_tensor[label]]\n",
    "\n",
    "            nms_out_index = torchvision.ops.nms(\n",
    "                box,\n",
    "                score,\n",
    "                self.nmsthre,\n",
    "            )\n",
    "            output_list.append(None)\n",
    "            \n",
    "            ############################################\n",
    "            result_box = box[nms_out_index].detach().cpu().numpy()\n",
    "            result_score = score[nms_out_index].detach().cpu().numpy()\n",
    "\n",
    "            for idx, bbox in enumerate(result_box):\n",
    "                if isinstance(bbox, type(None)):\n",
    "                    output_list.append(None)\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = map(int, bbox)\n",
    "                x1 = int((x1 * ori_w)/self.input_w)\n",
    "                y1 = int((y1 * ori_h)/self.input_h)\n",
    "                x2 = int((x2 * ori_w)/self.input_w)\n",
    "                y2 = int((y2 * ori_h)/self.input_h)\n",
    "                score = str(result_score[idx])\n",
    "                out = {\"bbox\":[x1, y1, x2, y2], \"score\":score, \"label\":label}\n",
    "                output_list.append(out)\n",
    "            ############################################\n",
    "\n",
    "        return output_list\n",
    "        \n",
    "\n",
    "\n",
    "    def preprocess(self,frame_batch) : \n",
    "\n",
    "#         input_data = torch.zeros([3, self.input_h, self.input_w], dtype=torch.float32, device=torch.device(\"cuda\")).fill_(144)\n",
    "        scale_list = []\n",
    "        \n",
    "        for idx, frame in enumerate(frame_batch) :\n",
    "            _, h, w = frame.shape\n",
    "            permute = [2, 1, 0]\n",
    "            frame = frame[permute,:,:]\n",
    "            resized_img = torchvision.transforms.functional.resize(frame, (self.input_h, self.input_w)).float()\n",
    "            input_data = resized_img.div(255.0)\n",
    "#             input_data[:,:self.input_h,:self.input_w] = resized_img \n",
    "            scale_list.append([h,w])\n",
    "        return input_data, scale_list\n",
    "\n",
    "    \n",
    "    def inference(self,input_data) : \n",
    "        output_data = self.Engine.do_inference_v2(input_data)\n",
    "        return output_data\n",
    "\n",
    "    \n",
    "    def parse_output(self,input_data_batch,output_batch,reference_CM):\n",
    "        res = []\n",
    "        for idx_i, data in enumerate(input_data_batch): \n",
    "            framedata = data['framedata']\n",
    "            scenario = data['scenario']\n",
    "            channel_id = framedata['meta']['source']['channel_id']\n",
    "            if output_batch == None:\n",
    "                input_data = dict()\n",
    "                input_data[\"framedata\"] = framedata\n",
    "                input_data[\"bbox\"] = None\n",
    "                input_data[\"scenario\"] = scenario   \n",
    "                input_data[\"data\"] = None\n",
    "                input_data[\"available\"] = False\n",
    "                res.append(input_data)\n",
    "                continue\n",
    "            for idx_j, output in enumerate(output_batch): \n",
    "\n",
    "                if isinstance(output, type(None)):\n",
    "                    input_data = dict()\n",
    "                    input_data[\"framedata\"] = framedata\n",
    "                    input_data[\"bbox\"] = None\n",
    "                    input_data[\"scenario\"] = scenario   \n",
    "                    input_data[\"data\"] = None   \n",
    "                    input_data[\"available\"] = False\n",
    "                    res.append(input_data)\n",
    "                    continue\n",
    "                    \n",
    "                label = int(output['label']) \n",
    "                frame_count = framedata['meta']['source']['frame_count']\n",
    "\n",
    "                if int(label) != 0 : \n",
    "                    input_data = dict()\n",
    "                    input_data[\"framedata\"] = framedata\n",
    "                    input_data[\"bbox\"] = None\n",
    "                    input_data[\"scenario\"] = scenario   \n",
    "                    input_data[\"data\"] = None   \n",
    "                    input_data[\"available\"] = False\n",
    "                    res.append(input_data)\n",
    "                    continue\n",
    "\n",
    "                input_data = dict()\n",
    "                input_data[\"framedata\"] = framedata\n",
    "                input_data[\"bbox\"] = output['bbox']\n",
    "                input_data[\"scenario\"] = scenario   \n",
    "                input_data[\"data\"] = {\"score\":output['score'], \"label\":str(output['label'])}\n",
    "                input_data[\"available\"] = True\n",
    "                res.append(input_data)\n",
    "        return res  \n",
    "        \n",
    "    def run_inference(self, input_data_batch, unavailable_routing_data_batch, reference_CM=None):\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        parsed_input_batch = self.parse_input(input_data_batch)\n",
    "        t2 = time.time()\n",
    "        \n",
    "        x,scale_list = self.preprocess(parsed_input_batch)\n",
    "        t3 = time.time()\n",
    "        \n",
    "        x = self.inference(x)\n",
    "        t4 = time.time()\n",
    "        \n",
    "        output_batch = self.postprocess(x,scale_list)\n",
    "        t5 = time.time()\n",
    "        \n",
    "\n",
    "        output = self.parse_output(input_data_batch,output_batch,reference_CM)\n",
    "        t6 = time.time()\n",
    "        frame_time = (t6 - t1) / len(input_data_batch)\n",
    "        \n",
    "#         print(f'[PEOPLE_DETECTOR] 1.parse_input : {t2 - t1}, 2.preprocess : {t3 - t2}, 3.inference : {t4 - t3}, 4.postprocess : {t5 - t4}, 5.parse_output : {t6 - t5},  6.total : {t6 - t1} 7. per_frame_time : {frame_time} 8. input_data_size : {len(input_data_batch)}')\n",
    "#         print(f'________________________________________________________________________')\n",
    "        \n",
    "        return output, unavailable_routing_data_batch\n",
    "\n",
    "\n",
    "    \n",
    "weights = '/DATA_17/ij/test/best_model_people.trt'\n",
    "\n",
    "img_path = '/DATA_17/ij/test/test_image.jpeg'\n",
    "# image = cv2.imread(img_path)[..., ::-1]#BGR 순서를 RGB로 뒤집습니다.\n",
    "image = cv2.imread(img_path)\n",
    "frame = np.copy(image)\n",
    "\n",
    "infe_frame = torch.from_numpy(frame).to(torch.device(\"cuda\"))\n",
    "infe_frame = infe_frame.permute(2, 0, 1) \n",
    "\n",
    "\n",
    "pe = PEOPLE_DETECTOR()\n",
    "pe.load(weights)\n",
    "\n",
    "\n",
    "input_data, scale_list = pe.preprocess([infe_frame])\n",
    "\n",
    "result = pe.inference(input_data)\n",
    "# print(f'result : {result[0].shape} {result[1].shape}')#result : torch.Size([12, 34, 60]) torch.Size([3, 34, 60])\n",
    "\n",
    "# output_batch = pe.postprocess(result, scale_list)\n",
    "\n",
    "# print(output_batch)\n",
    "\n",
    "# image = cv2.imread(img_path)\n",
    "\n",
    "\n",
    "# for result in output_batch:\n",
    "#     xmin, ymin, xmax, ymax = result['bbox']\n",
    "#     color = [255, 0, 0] \n",
    "#     cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e9582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_h = 544\n",
    "input_w = 960\n",
    "stride = 16\n",
    "box_norm = 35\n",
    "\n",
    "grid_h = 34\n",
    "grid_w = 60\n",
    "norm_data_x = torch.zeros([grid_h, grid_w], dtype=torch.float32, device=torch.device(\"cuda\"))\n",
    "norm_data_y = torch.zeros([grid_h, grid_w], dtype=torch.float32, device=torch.device(\"cuda\"))\n",
    "\n",
    "\n",
    "grid_h = int(input_h / stride)\n",
    "grid_w = int(input_w / stride)\n",
    "grid_size = grid_h * grid_w\n",
    "\n",
    "for i in range(grid_h):\n",
    "    value = (i * stride + 0.5) / box_norm\n",
    "    norm_data_y[i,:] = value\n",
    "\n",
    "\n",
    "for i in range(grid_w):\n",
    "    value = (i * stride + 0.5) / box_norm\n",
    "    norm_data_x[:,i] = value\n",
    "\n",
    "# norm_data_x = torch.stack((norm_data_x,norm_data_x,norm_data_x))\n",
    "# norm_data_y = torch.stack((norm_data_y,norm_data_y,norm_data_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "96e950ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.7571e+01, -4.2035e+01, -6.6948e+01,  ...,  8.1019e+02,\n",
       "           8.6032e+02,  9.4193e+02],\n",
       "         [-4.5277e+01, -8.1832e+01, -1.5286e+02,  ...,  7.6436e+02,\n",
       "           8.6874e+02,  9.4603e+02],\n",
       "         [-1.3719e+01, -5.0880e+01, -8.6292e+01,  ...,  8.3353e+02,\n",
       "           9.0191e+02,  9.4676e+02],\n",
       "         ...,\n",
       "         [-3.0834e+01, -8.6500e-01,  1.2328e+01,  ...,  9.0063e+02,\n",
       "           9.1344e+02,  9.4133e+02],\n",
       "         [-6.0671e+00,  4.2042e+00,  1.6960e+01,  ...,  9.0376e+02,\n",
       "           9.1462e+02,  9.4244e+02],\n",
       "         [-1.1248e+01,  5.2154e+00,  1.2388e+01,  ...,  9.0657e+02,\n",
       "           9.1830e+02,  9.4237e+02]],\n",
       "\n",
       "        [[-1.6456e+01, -1.9195e+01, -5.2801e+01,  ...,  8.4705e+02,\n",
       "           8.7772e+02,  9.4952e+02],\n",
       "         [-2.6306e+01, -7.4261e+01, -1.0313e+02,  ...,  8.5062e+02,\n",
       "           8.8776e+02,  9.4362e+02],\n",
       "         [-3.5102e+01, -6.6522e+01, -6.8397e+01,  ...,  8.5851e+02,\n",
       "           8.6973e+02,  9.3407e+02],\n",
       "         ...,\n",
       "         [-1.3648e+01, -6.2545e+00, -9.3248e+00,  ...,  8.6162e+02,\n",
       "           9.0526e+02,  9.4816e+02],\n",
       "         [-1.0932e+01, -2.1623e+00, -6.1787e+00,  ...,  8.8288e+02,\n",
       "           9.0830e+02,  9.4202e+02],\n",
       "         [-6.2976e+00,  2.7313e+00,  8.7828e+00,  ...,  9.0433e+02,\n",
       "           9.1914e+02,  9.4387e+02]],\n",
       "\n",
       "        [[ 1.7643e+00,  1.8268e+01,  2.5749e+01,  ...,  9.0455e+02,\n",
       "           9.1670e+02,  9.5081e+02],\n",
       "         [-9.7545e+00,  1.1040e+01,  3.1887e+01,  ...,  9.0930e+02,\n",
       "           9.2638e+02,  9.4828e+02],\n",
       "         [-9.7792e+00, -3.7051e+00,  1.9011e+01,  ...,  9.1177e+02,\n",
       "           9.2592e+02,  9.4830e+02],\n",
       "         ...,\n",
       "         [-1.5892e+00,  1.4442e+01,  3.0887e+01,  ...,  9.0972e+02,\n",
       "           9.3195e+02,  9.4969e+02],\n",
       "         [ 8.5369e+00,  1.2777e+01,  1.9757e+01,  ...,  9.1380e+02,\n",
       "           9.3375e+02,  9.5044e+02],\n",
       "         [ 1.4745e+01,  2.0615e+01,  2.0942e+01,  ...,  9.1771e+02,\n",
       "           9.3384e+02,  9.4596e+02]]], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_x1 = torch.stack((result[0][0],result[0][4],result[0][8]))\n",
    "tensor_y1 = torch.stack((result[0][1],result[0][5],result[0][9]))\n",
    "tensor_x2 = torch.stack((result[0][2],result[0][6],result[0][10]))\n",
    "tensor_y2 = torch.stack((result[0][3],result[0][7],result[0][11]))\n",
    "\n",
    "tensor_x1 = (tensor_x1 - norm_data_x) * -35\n",
    "tensor_y1 = (tensor_y1 - norm_data_y) * -35\n",
    "tensor_x2 = (tensor_x2 + norm_data_x) * 35\n",
    "tensor_y2 = (tensor_y2 + norm_data_y) * 35\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c244ac59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19188308715820312\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s = time.time()\n",
    "for i in range(1000) : \n",
    "    tensor_x1 = torch.cat([result[0][0],result[0][4],result[0][8]],0).view(-1,grid_h,grid_w)\n",
    "    tensor_y1 = torch.cat([result[0][1],result[0][5],result[0][9]],0).view(-1,grid_h,grid_w)\n",
    "    tensor_x2 = torch.cat([result[0][2],result[0][6],result[0][10]],0).view(-1,grid_h,grid_w)\n",
    "    tensor_y2 = torch.cat([result[0][3],result[0][7],result[0][11]],0).view(-1,grid_h,grid_w)\n",
    "    tensor_x1 = (tensor_x1 - norm_data_x) * -35\n",
    "    tensor_y1 = (tensor_y1 - norm_data_y) * -35\n",
    "    tensor_x2 = (tensor_x2 + norm_data_x) * 35\n",
    "    tensor_y2 = (tensor_y2 + norm_data_y) * 35\n",
    "\n",
    "print(time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62cdadf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35172414779663086\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "for i in range(1000) : \n",
    "\n",
    "    tensor_x1 = (result[0][0] - norm_data_x) * -35\n",
    "    tensor_x2 = (result[0][4] - norm_data_x) * -35\n",
    "    tensor_x3 = (result[0][8] - norm_data_x) * -35\n",
    "\n",
    "    \n",
    "    tensor_y1 = (result[0][1] - norm_data_y) * -35\n",
    "    tensor_y2 = (result[0][5] - norm_data_y) * -35\n",
    "    tensor_y3 = (result[0][9] - norm_data_y) * -35\n",
    "\n",
    "                \n",
    "    tensor_x21 = (result[0][2] + norm_data_x) * 35\n",
    "    tensor_x22 = (result[0][6] + norm_data_x) * 35\n",
    "    tensor_x23 = (result[0][10] + norm_data_x) * 35\n",
    "    \n",
    "    tensor_y21 = (result[0][3] + norm_data_y) * 35\n",
    "    tensor_y22 = (result[0][7] + norm_data_y) * 35\n",
    "    tensor_y23 = (result[0][11] + norm_data_y) * 35\n",
    "\n",
    "print(time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1adef316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00019782114028930664\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "for i in range(1000) : \n",
    "    tensor_x1 = torch.stack((result[0][0],result[0][4],result[0][8]))\n",
    "    tensor_y1 = torch.stack((result[0][1],result[0][5],result[0][9]))\n",
    "    tensor_x2 = torch.stack((result[0][2],result[0][6],result[0][10]))\n",
    "    tensor_y2 = torch.stack((result[0][3],result[0][7],result[0][11]))\n",
    "\n",
    "    tensor_x1 = (tensor_x1 - norm_data_x) * -35\n",
    "    tensor_y1 = (tensor_y1 - norm_data_y) * -35\n",
    "    tensor_x2 = (tensor_x2 + norm_data_x) * 35\n",
    "    tensor_y2 = (tensor_y2 + norm_data_y) * 35\n",
    "\n",
    "\n",
    "print((time.time()-s)/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41d8cc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0007558000087738037\n"
     ]
    }
   ],
   "source": [
    "score_tensor = result[1] >=0.3\n",
    "output = []\n",
    "\n",
    "s = time.time()\n",
    "for i in range(1000) : \n",
    "\n",
    "    for label in range(3): \n",
    "        new_x1 = tensor_x1[label][score_tensor[label]]\n",
    "        new_y1 = tensor_y1[label][score_tensor[label]]\n",
    "        new_x2 = tensor_x2[label][score_tensor[label]]\n",
    "        new_y2 = tensor_y2[label][score_tensor[label]]\n",
    "print((time.time()-s)/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "abf7beb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001633723020553589\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "s1 = time.time()\n",
    "for i in range(1000) : \n",
    "\n",
    "    tensor_x1 = torch.stack((result[0][0],result[0][4],result[0][8]))\n",
    "    tensor_y1 = torch.stack((result[0][1],result[0][5],result[0][9]))\n",
    "    tensor_x2 = torch.stack((result[0][2],result[0][6],result[0][10]))\n",
    "    tensor_y2 = torch.stack((result[0][3],result[0][7],result[0][11]))\n",
    "\n",
    "    tensor_x1 = (tensor_x1 - norm_data_x) * -35\n",
    "    tensor_y1 = (tensor_y1 - norm_data_y) * -35\n",
    "    tensor_x2 = (tensor_x2 + norm_data_x) * 35\n",
    "    tensor_y2 = (tensor_y2 + norm_data_y) * 35\n",
    "\n",
    "\n",
    "\n",
    "    score_tensor = result[1] >=0.3\n",
    "    output = []\n",
    "\n",
    "    ori_w = 640\n",
    "    ori_h = 426\n",
    "\n",
    "    for label in range(3) : \n",
    "\n",
    "        new_x1 = tensor_x1[label][score_tensor[label]]\n",
    "        new_y1 = tensor_y1[label][score_tensor[label]]\n",
    "        new_x2 = tensor_x2[label][score_tensor[label]]\n",
    "        new_y2 = tensor_y2[label][score_tensor[label]]\n",
    "\n",
    "\n",
    "    #     box = torch.stack((new_x1,new_y1,new_x2,new_y2),1)\n",
    "\n",
    "        box = torch.stack((new_x1,new_y1,new_x2,new_y2),1)\n",
    "\n",
    "\n",
    "        score = result[1][label][score_tensor[label]]\n",
    "        s2 = time.time()\n",
    "        nms_out_index = torchvision.ops.nms(\n",
    "            box,\n",
    "            score,\n",
    "            0.5,\n",
    "        )\n",
    "print((time.time()-s1)/1000)\n",
    "\n",
    "#     result_box = box[nms_out_index].detach().cpu().numpy()\n",
    "#     result_score = score[nms_out_index].detach().cpu().numpy()\n",
    "\n",
    "#     for idx, bbox in enumerate(result_box):\n",
    "#         if isinstance(bbox, type(None)):\n",
    "#             output.append(None)\n",
    "#             continue\n",
    "#         x1, y1, x2, y2 = map(int, bbox)\n",
    "#         x1 = int((x1 * ori_w)/input_w)\n",
    "#         y1 = int((y1 * ori_h)/input_h)\n",
    "#         x2 = int((x2 * ori_w)/input_w)\n",
    "#         y2 = int((y2 * ori_h)/input_h)\n",
    "#         score = str(result_score[idx])\n",
    "#         output.append({\"bbox\":[x1, y1, x2, y2], \"score\":score, \"label\":label})\n",
    "\n",
    "\n",
    "\n",
    "# img_path = '/DATA_17/ij/test/test_image.jpeg'\n",
    "# image = cv2.imread(img_path)\n",
    "# for result in output:\n",
    "#     xmin, ymin, xmax, ymax = result['bbox']\n",
    "#     color = [255, 0, 0] \n",
    "#     cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617d040a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7695e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.FloatTensor([[1., 2., 3.],\n",
    "                       [4., 5., 6.],\n",
    "                       [7., 8., 9.],\n",
    "                       [10., 11., 12.]\n",
    "                      ])\n",
    "t2 = torch.FloatTensor([[11., 12., 13.],\n",
    "                       [14., 15., 16.],\n",
    "                       [17., 18., 19.],\n",
    "                       [27., 21., 22.]\n",
    "                      ])\n",
    "s1 = time.time()\n",
    "a = torch.stack((t1, t2))\n",
    "print(a.shape, time.time()-s1)\n",
    "\n",
    "s2 = time.time()\n",
    "d = torch.cat([t1,t2], 0).view(-1,4,3) \n",
    "print(d.shape, time.time()-s2)\n",
    "\n",
    "\n",
    "# c,d = torch.max(a, 1, keepdim=True)\n",
    "# # b = a.ravel()\n",
    "# # a[0,3,2]\n",
    "\n",
    "# # c = a[0]\n",
    "# # d = a[1]\n",
    "\n",
    "# # d.shape\n",
    "# print(c)\n",
    "# print(d)\n",
    "# print(c.shape)\n",
    "# print(d.shape)\n",
    "\n",
    "# b = a > 10 \n",
    "\n",
    "# a[0][b[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644b198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0.014285714285714285, 0.4714285714285714, 0.9285714285714286, 1.3857142857142857, 1.8428571428571427, 2.3, 2.757142857142857, 3.2142857142857144, 3.6714285714285713, 4.128571428571429, 4.585714285714285, 5.042857142857143, 5.5, 5.957142857142857, 6.414285714285715, 6.871428571428571, 7.328571428571428, 7.785714285714286, 8.242857142857142, 8.7, 9.157142857142857, 9.614285714285714, 10.071428571428571, 10.528571428571428, 10.985714285714286, 11.442857142857143, 11.9, 12.357142857142858, 12.814285714285715, 13.271428571428572, 13.728571428571428, 14.185714285714285, 14.642857142857142, 15.1, 15.557142857142857, 16.014285714285716, 16.47142857142857, 16.928571428571427, 17.385714285714286, 17.84285714285714, 18.3, 18.757142857142856, 19.214285714285715, 19.67142857142857, 20.12857142857143, 20.585714285714285, 21.042857142857144, 21.5, 21.957142857142856, 22.414285714285715, 22.87142857142857, 23.32857142857143, 23.785714285714285, 24.242857142857144, 24.7, 25.15714285714286, 25.614285714285714, 26.071428571428573, 26.52857142857143, 26.985714285714284]\n",
    "b = [0.014285714285714285, 0.4714285714285714, 0.9285714285714286, 1.3857142857142857, 1.8428571428571427, 2.3, 2.757142857142857, 3.2142857142857144, 3.6714285714285713, 4.128571428571429, 4.585714285714285, 5.042857142857143, 5.5, 5.957142857142857, 6.414285714285715, 6.871428571428571, 7.328571428571428, 7.785714285714286, 8.242857142857142, 8.7, 9.157142857142857, 9.614285714285714, 10.071428571428571, 10.528571428571428, 10.985714285714286, 11.442857142857143, 11.9, 12.357142857142858, 12.814285714285715, 13.271428571428572, 13.728571428571428, 14.185714285714285, 14.642857142857142, 15.1]\n",
    "len(a\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0a1163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f9a943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70463017",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0.7430, 0.8202, 0.8331, 0.8958, 0.8933, 0.7453, 0.8693, 0.8905, 0.8900,\n",
    "        0.8025, 0.8258, 0.8339, 0.6816, 0.8935, 0.8963, 0.8787, 0.8909, 0.8937,\n",
    "        0.8951, 0.8220, 0.8422, 0.8935, 0.8947, 0.5792, 0.8929, 0.8936, 0.8319]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33b1023",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.tensor(a)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd2c23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.FloatTensor([[1., 2., 3.],\n",
    "                       [4., 5., 6.],\n",
    "                       [7., 8., 9.],\n",
    "                       [10., 11., 12.]\n",
    "                      ])\n",
    "t2 = torch.FloatTensor([[11., 12., 13.],\n",
    "                       [14., 15., 16.],\n",
    "                       [17., 18., 19.],\n",
    "                       [27., 21., 22.]\n",
    "                      ])\n",
    "\n",
    "\n",
    "d = torch.cat([t1,t2], 0).view(2,4,3) \n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78591248",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = list(range(3))\n",
    "for i in num_classes : \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c419ca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/DATA_17/ij/test/test_image.jpeg'\n",
    "image = cv2.imread(img_path)\n",
    "\n",
    "# image = cv2.imread(img_path)[..., ::-1]#BGR 순서를 RGB로 뒤집습니다.\n",
    "image = cv2.imread(img_path)\n",
    "image = np.copy(image)\n",
    "image = torch.from_numpy(image).to(torch.device(\"cuda\"))\n",
    "image = image.permute(2,0,1)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615699b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2,3],\n",
    "                  [4,5,6]])\n",
    "b = torch.tensor([[10,20,30],\n",
    "                  [40,50,60]])\n",
    "c = torch.cat([a,b],dim=1)\n",
    "e = torch.stack((a,b),1)\n",
    "d = c.view([-1,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df33ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
