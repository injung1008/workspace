{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99f82a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TensorRT] WARNING: Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data.shape 1  torch.Size([1566720]) [[426, 640]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TensorRT] WARNING: TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.2.0\n",
      "[TensorRT] WARNING: TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.2.0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "import cv2\n",
    "import torchvision\n",
    "import torch\n",
    "import time\n",
    "import traceback\n",
    "import importlib\n",
    "import common_people as common\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "torch.cuda.init()\n",
    "\n",
    "\n",
    "class PEOPLE_DETECTOR:\n",
    "    def __init__(self):  \n",
    "#     def __init__(self, logger):  \n",
    "#         self.logger = logger\n",
    "\n",
    "        \n",
    "        self.num_classes = list(range(1))\n",
    "        self.threshold = 0.3\n",
    "\n",
    "\n",
    "        self.input_h = 544\n",
    "        self.input_w = 960\n",
    "        self.wh_format = False\n",
    "\n",
    "        self.stride = 16\n",
    "        self.box_norm = 35.0\n",
    "        self.grid_calculator()\n",
    "\n",
    "    def load(self, weights):\n",
    "        self.weights = weights\n",
    "        self.Engine = common.Engine()      \n",
    "        self.Engine.make_context(self.weights)\n",
    "        self.batchsize = 1\n",
    "\n",
    "    def parse_input(self,input_data_batch):\n",
    "        res = []\n",
    "        for input_data in input_data_batch:\n",
    "            frame = input_data['framedata']['frame']\n",
    "            bbox = input_data['bbox']\n",
    "            cropped_img = common.getCropByFrame(frame,bbox)\n",
    "            res.append(cropped_img)\n",
    "        return res\n",
    "    \n",
    "    \n",
    "    def grid_calculator(self):\n",
    "        self.grid_h = int(self.input_h / self.stride)\n",
    "        self.grid_w = int(self.input_w / self.stride)\n",
    "        self.grid_size = self.grid_h * self.grid_w\n",
    "\n",
    "        self.grid_centers_w = []\n",
    "        self.grid_centers_h = []\n",
    "\n",
    "        for i in range(self.grid_h):\n",
    "            value = (i * self.stride + 0.5) / self.box_norm\n",
    "            self.grid_centers_h.append(value)\n",
    "\n",
    "        for i in range(self.grid_w):\n",
    "            value = (i * self.stride + 0.5) / self.box_norm\n",
    "            self.grid_centers_w.append(value)\n",
    "        \n",
    "#         print(f'self.grid_h : {self.grid_h}, self.grid_w : {self.grid_w}, self.grid_size : {self.grid_size}')\n",
    "#         print(f'self.grid_centers_w : {self.grid_centers_w}, self.grid_centers_h : {self.grid_centers_h}')\n",
    "\n",
    "    \n",
    "    # 박스 nomalize\n",
    "    def applyBoxNorm(self,o1, o2, o3, o4, x, y):\n",
    "        \"\"\"\n",
    "        Applies the GridNet box normalization\n",
    "        Args:\n",
    "            o1 (float): first argument of the result\n",
    "            o2 (float): second argument of the result\n",
    "            o3 (float): third argument of the result\n",
    "            o4 (float): fourth argument of the result\n",
    "            x: row index on the grid\n",
    "            y: column index on the grid\n",
    "\n",
    "        Returns:\n",
    "            float: rescaled first argument\n",
    "            float: rescaled second argument\n",
    "            float: rescaled third argument\n",
    "            float: rescaled fourth argument\n",
    "        \"\"\"\n",
    "#         print(f'o1, o2, o3, o4 before : {o1}')\n",
    "        o1 = (o1 - self.grid_centers_w[x]) * -self.box_norm\n",
    "        o2 = (o2 - self.grid_centers_h[y]) * -self.box_norm\n",
    "        o3 = (o3 + self.grid_centers_w[x]) * self.box_norm\n",
    "        o4 = (o4 + self.grid_centers_h[y]) * self.box_norm\n",
    "#         print(f'o1, o2, o3, o4 after: {o1}')\n",
    "        \n",
    "#         o1, o2, o3, o4 before : (1.0848912, 0.9234038, 0.86667395, 1.4356118, 44, 7)\n",
    "#         o1, o2, o3, o4 after: (666.5288079977036, 80.18086701631546, 734.8335881233215, 162.74641454219818)\n",
    "    \n",
    "    \n",
    "        return o1, o2, o3, o4\n",
    "    \n",
    "    def NMSBoxes(self, bboxes,scores,class_ids, outputs_list,scale_list):\n",
    "        indexes = cv2.dnn.NMSBoxes(bboxes, scores, self.threshold, 0.5)\n",
    "        print('indexes',indexes)\n",
    "        ori_h = scale_list[0][0]\n",
    "        ori_w = scale_list[0][1]\n",
    "        \n",
    "        for idx in indexes:\n",
    "            idx = int(idx)\n",
    "            xmin, ymin, w, h = bboxes[idx]\n",
    "            xmax = w + xmin\n",
    "            ymax = h + ymin\n",
    "            print('ccccccc', xmin, ymin, w, h)\n",
    "            ori_x_min = int((xmin * ori_w)/self.input_w)\n",
    "            ori_y_min = int((ymin * ori_h)/self.input_h)\n",
    "            \n",
    "            ori_x_max = int((xmax * ori_w)/self.input_w)\n",
    "            ori_y_max = int((ymax * ori_h)/self.input_h)\n",
    "            \n",
    "            \n",
    "            class_id = class_ids[idx]\n",
    "            score = scores[idx]\n",
    "            output = {\"bbox\":[ori_x_min, ori_y_min, ori_x_max, ori_y_max], \"score\":score, \"label\":class_id}\n",
    "            outputs_list.append(output)\n",
    "        return outputs_list\n",
    "\n",
    "    def postprocess(self,outputs, scale_list):\n",
    "        \"\"\"\n",
    "        Postprocesses the inference output\n",
    "        Args:\n",
    "            outputs (list of float): inference output\n",
    "            min_confidence (float): min confidence to accept detection\n",
    "            analysis_classes (list of int): indices of the classes to consider\n",
    "\n",
    "        Returns: list of list tuple: each element is a two list tuple (x, y) representing the corners of a bb\n",
    "        \"\"\"\n",
    "        t1 = time.time()\n",
    "        outputs[0] = outputs[0].detach().cpu().numpy()\n",
    "        outputs[1] = outputs[1].detach().cpu().numpy()\n",
    "        print(f\"outputs[0].shape : {outputs[0].shape}\")\n",
    "        print(f\"outputs[1].shape : {outputs[1].shape}\")\n",
    "        \n",
    "        t2 = time.time()\n",
    "        outputs_list = []\n",
    "        bbs = []\n",
    "        class_ids = []\n",
    "        scores = []\n",
    "        if isinstance(outputs[0], type(None)):\n",
    "            return\n",
    "        \n",
    "        \n",
    "        t3 = time.time()  \n",
    "        boxes = outputs[0]\n",
    "        \n",
    "#         print(f\"bbox : {boxes}\")\n",
    "#         print(f\"bbox.shape : {boxes.shape}\")   \n",
    "        \n",
    "#         self.grid_h : 34, self.grid_w : 60, self.grid_size : 2040\n",
    "#       torch.Size([12, 34, 60]) -> 24480\n",
    "        #[0, 1, 2] \n",
    "        for c in self.num_classes:\n",
    "            print(f'c : {c}')\n",
    "            x1_idx = c * 4 * self.grid_size #self.grid_size : 2040\n",
    "            y1_idx = x1_idx + self.grid_size\n",
    "            x2_idx = y1_idx + self.grid_size\n",
    "            y2_idx = x2_idx + self.grid_size\n",
    "#             print(f'c : {c} x1_idx : {x1_idx}, y1_idx : {y1_idx}, x2_idx : {x2_idx} y2_idx : {y2_idx}')\n",
    "#             c : 0 x1_idx : 0, y1_idx : 2040, x2_idx : 4080 y2_idx : 6120\n",
    "#             c : 1 x1_idx : 8160, y1_idx : 10200, x2_idx : 12240 y2_idx : 14280\n",
    "#             c : 2 x1_idx : 16320, y1_idx : 18360, x2_idx : 20400 y2_idx : 22440\n",
    "#                 y2_idx : 22440 + 2040 = 24480 -> outputs[0]의 tensor갯수\n",
    "\n",
    "                \n",
    "            t3_1 = time.time()\n",
    "            for h in range(self.grid_h):\n",
    "                for w in range(self.grid_w):\n",
    "                    i = w + h * self.grid_w\n",
    "#                     print(f'w : {w}, h : {h}, self.grid_w : {self.grid_w}, i : {i}')\n",
    "#                     print(f'c * self.grid_size + i : {c * self.grid_size + i}')\n",
    "#                     i : 0 - 2039\n",
    "#                     c * self.grid_size + i : 0 - 6119\n",
    "#                     print(f'i : {i}, w : {w}, h: {h}')\n",
    "                    score = outputs[1][c * self.grid_size + i]\n",
    "#                     print(f'score : {score}')\n",
    "                    if score >= self.threshold:\n",
    "                        o1 = boxes[x1_idx + w + h * self.grid_w]\n",
    "                        o2 = boxes[y1_idx + w + h * self.grid_w]\n",
    "                        o3 = boxes[x2_idx + w + h * self.grid_w]\n",
    "                        o4 = boxes[y2_idx + w + h * self.grid_w]\n",
    "                    #  i == 0 -> 0,2040,4080,6120\n",
    "                    #  i == 1 -> 1,2041,4081,6121\n",
    "\n",
    "                        o1, o2, o3, o4 = self.applyBoxNorm(o1, o2, o3, o4, w, h)\n",
    "\n",
    "                        xmin = int(o1)\n",
    "                        ymin = int(o2)\n",
    "                        xmax = int(o3)\n",
    "                        ymax = int(o4)\n",
    "                        if self.wh_format:\n",
    "                            bbs.append([xmin, ymin, xmax - xmin, ymax - ymin])\n",
    "                        else:\n",
    "                            bbs.append([xmin, ymin, xmax, ymax])\n",
    "                        class_ids.append(c)\n",
    "                        scores.append(float(score))\n",
    "            \n",
    "            t3_2 = time.time()\n",
    "            print(f\"t3_2 ~ t3_1 : {t3_2 - t3_1}\")\n",
    "\n",
    "        t4 = time.time()      \n",
    "        print(f'bbs {bbs}')\n",
    "        outputs_list = self.NMSBoxes(bbs,scores,class_ids,outputs_list,scale_list)\n",
    "        t5 = time.time()\n",
    "        \n",
    "        print(f\"t1~t2 : {t2-t1}\")\n",
    "        print(f\"t2~t3 : {t3-t2}\")\n",
    "        print(f\"t3~t4 : {t4-t3}\")\n",
    "        print(f\"t4~t5 : {t5-t4}\")\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        return outputs_list\n",
    "\n",
    "    def preprocess(self,frame_batch) : \n",
    "\n",
    "        input_data = torch.zeros([3, self.input_h, self.input_w], dtype=torch.float32, device=torch.device(\"cuda\")).fill_(144)\n",
    "        scale_list = []\n",
    "        \n",
    "        for idx, frame in enumerate(frame_batch) :\n",
    "            _, h, w = frame.shape\n",
    "            permute = [2, 1, 0]\n",
    "            frame = frame[permute,:,:]\n",
    "            resized_img = torchvision.transforms.functional.resize(frame, (self.input_h, self.input_w)).float()\n",
    "            resized_img = resized_img.div(255.0)\n",
    "            input_data[:,:self.input_h,:self.input_w] = resized_img \n",
    "            input_data = torch.ravel(input_data)\n",
    "            scale_list.append([h,w])\n",
    "        print('input_data.shape 1 ',input_data.shape,scale_list)\n",
    "        return input_data, scale_list\n",
    "\n",
    "    \n",
    "    def inference(self,input_data) : \n",
    "        output_data = self.Engine.do_inference_v2(input_data)\n",
    "        return output_data\n",
    "\n",
    "    \n",
    "#     def parse_output(self,input_data_batch,output_batch,reference_CM):\n",
    "    def parse_output(self,output_batch):\n",
    "        res = []\n",
    "        idx_i = 0\n",
    "        for idx_i, data in enumerate(input_data_batch): \n",
    "            if output_batch == None:\n",
    "                input_data = dict()\n",
    "                input_data[\"framedata\"] = framedata\n",
    "                input_data[\"bbox\"] = None\n",
    "                input_data[\"scenario\"] = scenario   \n",
    "                input_data[\"data\"] = None\n",
    "                input_data[\"available\"] = False\n",
    "                res.append(input_data)\n",
    "                continue\n",
    "            for idx_j, output in enumerate(output_batch): \n",
    "                print('output',output)\n",
    "                if isinstance(output, type(None)):\n",
    "                    input_data = dict()\n",
    "                    input_data[\"framedata\"] = framedata\n",
    "                    input_data[\"bbox\"] = None\n",
    "                    input_data[\"scenario\"] = scenario   \n",
    "                    input_data[\"data\"] = None   \n",
    "                    input_data[\"available\"] = False\n",
    "                    res.append(input_data)\n",
    "                    continue\n",
    "\n",
    "                input_data = dict()\n",
    "                input_data[\"framedata\"] = framedata\n",
    "                input_data[\"bbox\"] = output['bbox']\n",
    "                input_data[\"scenario\"] = scenario   \n",
    "                input_data[\"data\"] = {\"score\":output['score'], \"label\":str(label)}\n",
    "                input_data[\"available\"] = True\n",
    "                res.append(input_data)\n",
    "        return res  \n",
    "        \n",
    "   \n",
    "    \n",
    "weights = '/DATA_17/ij/test/best_model_people.trt'\n",
    "\n",
    "img_path = '/DATA_17/ij/test/test_image.jpeg'\n",
    "# image = cv2.imread(img_path)[..., ::-1]#BGR 순서를 RGB로 뒤집습니다.\n",
    "image = cv2.imread(img_path)\n",
    "image = np.copy(image)\n",
    "image = torch.from_numpy(image).to(torch.device(\"cuda\"))\n",
    "image = image.permute(2,0,1)\n",
    "frame_batch = [image]\n",
    "\n",
    "\n",
    "\n",
    "pe = PEOPLE_DETECTOR()\n",
    "pe.load(weights)\n",
    "\n",
    "\n",
    "input_data, scale_list = pe.preprocess(frame_batch)\n",
    "\n",
    "result = pe.inference(input_data)\n",
    "\n",
    "\n",
    "# output_batch = pe.postprocess(result, scale_list)\n",
    "\n",
    "# print(output_batch)\n",
    "\n",
    "# image = cv2.imread(img_path)\n",
    "\n",
    "\n",
    "# for result in output_batch:\n",
    "#     xmin, ymin, xmax, ymax = result['bbox']\n",
    "#     color = [255, 0, 0] \n",
    "#     cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ffc0956",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_h = 544\n",
    "input_w = 960\n",
    "stride = 16\n",
    "box_norm = 35\n",
    "\n",
    "grid_h = 34\n",
    "grid_w = 60\n",
    "norm_data_x = torch.zeros([grid_h, grid_w], dtype=torch.float32, device=torch.device(\"cuda\"))\n",
    "norm_data_y = torch.zeros([grid_h, grid_w], dtype=torch.float32, device=torch.device(\"cuda\"))\n",
    "\n",
    "\n",
    "grid_h = int(input_h / stride)\n",
    "grid_w = int(input_w / stride)\n",
    "grid_size = grid_h * grid_w\n",
    "\n",
    "for i in range(grid_h):\n",
    "    value = (i * stride + 0.5) / box_norm\n",
    "    norm_data_y[i,:] = value\n",
    "\n",
    "\n",
    "for i in range(grid_w):\n",
    "    value = (i * stride + 0.5) / box_norm\n",
    "    norm_data_x[:,i] = value\n",
    "\n",
    "norm_data_x = norm_data_x.view(-1,)\n",
    "norm_data_y = norm_data_y.view(-1,)\n",
    "norm_data_x = torch.cat([norm_data_x,norm_data_x,norm_data_x],-1)\n",
    "norm_data_y = torch.cat([norm_data_y,norm_data_y,norm_data_y],-1)\n",
    "\n",
    "label_c0 = torch.zeros([2040], dtype=torch.float32, device=torch.device(\"cuda\")).fill_(0)\n",
    "label_c1 = torch.zeros([2040], dtype=torch.float32, device=torch.device(\"cuda\")).fill_(1)\n",
    "label_c2 = torch.zeros([2040], dtype=torch.float32, device=torch.device(\"cuda\")).fill_(2)\n",
    "label_tensor = torch.cat([label_c0,label_c1,label_c2],-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a78889d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if sys.path[0] == \"\":\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0030083656311035156\n",
      "0.0013453960418701172\n",
      "0.0008940696716308594\n",
      "0.0008788108825683594\n",
      "0.0008776187896728516\n",
      "0.0008680820465087891\n",
      "0.0008647441864013672\n",
      "0.0008616447448730469\n",
      "0.0008752346038818359\n",
      "0.0011472702026367188\n",
      "0.0009279251098632812\n",
      "0.0008893013000488281\n",
      "0.0008933544158935547\n",
      "0.0008711814880371094\n",
      "0.0008633136749267578\n",
      "0.0008616447448730469\n",
      "0.00086212158203125\n",
      "0.000873565673828125\n",
      "0.0008616447448730469\n",
      "0.0008616447448730469\n",
      "0.0008623600006103516\n",
      "0.0008673667907714844\n",
      "0.0008599758148193359\n",
      "0.0008587837219238281\n",
      "0.0008592605590820312\n",
      "0.0008676052093505859\n",
      "0.0008914470672607422\n",
      "0.0008614063262939453\n",
      "0.0008587837219238281\n",
      "0.0008552074432373047\n",
      "0.0008647441864013672\n",
      "0.0008597373962402344\n",
      "0.0008606910705566406\n",
      "0.0008606910705566406\n",
      "0.0008573532104492188\n",
      "0.0008883476257324219\n",
      "0.0008640289306640625\n",
      "0.0008587837219238281\n",
      "0.000858306884765625\n",
      "0.000858306884765625\n",
      "0.0008931159973144531\n",
      "0.0008618831634521484\n",
      "0.0008552074432373047\n",
      "0.0008592605590820312\n",
      "0.0008738040924072266\n",
      "0.0008587837219238281\n",
      "0.0008571147918701172\n",
      "0.0008566379547119141\n",
      "0.0008597373962402344\n",
      "0.0008745193481445312\n",
      "0.0008575916290283203\n",
      "0.00086212158203125\n",
      "0.0008575916290283203\n",
      "0.00086212158203125\n",
      "0.0008592605590820312\n",
      "0.0008592605590820312\n",
      "0.0008573532104492188\n",
      "0.0008578300476074219\n",
      "0.0008711814880371094\n",
      "0.0008609294891357422\n",
      "0.0008587837219238281\n",
      "0.0008556842803955078\n",
      "0.0008711814880371094\n",
      "0.0008623600006103516\n",
      "0.0008590221405029297\n",
      "0.0008585453033447266\n",
      "0.000858306884765625\n",
      "0.0008711814880371094\n",
      "0.0008602142333984375\n",
      "0.0008571147918701172\n",
      "0.0008552074432373047\n",
      "0.0008575916290283203\n",
      "0.0008692741394042969\n",
      "0.000858306884765625\n",
      "0.0008566379547119141\n",
      "0.0008556842803955078\n",
      "0.0008668899536132812\n",
      "0.0008552074432373047\n",
      "0.000858306884765625\n",
      "0.0008571147918701172\n",
      "0.0008568763732910156\n",
      "0.0008704662322998047\n",
      "0.0008611679077148438\n",
      "0.0008585453033447266\n",
      "0.0008573532104492188\n",
      "0.0008640289306640625\n",
      "0.0008590221405029297\n",
      "0.0008578300476074219\n",
      "0.0008573532104492188\n",
      "0.00084686279296875\n",
      "0.0008664131164550781\n",
      "0.0008509159088134766\n",
      "0.0008544921875\n",
      "0.0008587837219238281\n",
      "0.0008547306060791016\n",
      "0.0009319782257080078\n",
      "0.0008594989776611328\n",
      "0.0008571147918701172\n",
      "0.0008592605590820312\n",
      "0.0008664131164550781\n",
      "0.0008561611175537109\n",
      "0.0008590221405029297\n",
      "0.0008599758148193359\n",
      "0.0008587837219238281\n",
      "0.0009489059448242188\n",
      "0.0008692741394042969\n",
      "0.0008625984191894531\n",
      "0.0008597373962402344\n",
      "0.0008685588836669922\n",
      "0.0008587837219238281\n",
      "0.0008599758148193359\n",
      "0.0008568763732910156\n",
      "0.0008571147918701172\n",
      "0.003634214401245117\n",
      "0.0008721351623535156\n",
      "0.0008597373962402344\n",
      "0.0008561611175537109\n",
      "0.0008602142333984375\n",
      "0.0008580684661865234\n",
      "0.0008671283721923828\n",
      "0.0008587837219238281\n",
      "0.0008575916290283203\n",
      "0.0008594989776611328\n",
      "0.0008695125579833984\n",
      "0.0008578300476074219\n",
      "0.0008587837219238281\n",
      "0.0008573532104492188\n",
      "0.0008585453033447266\n",
      "0.0008678436279296875\n",
      "0.0008609294891357422\n",
      "0.0008602142333984375\n",
      "0.0008604526519775391\n",
      "0.0008635520935058594\n",
      "0.0008590221405029297\n",
      "0.0008590221405029297\n",
      "0.0008563995361328125\n",
      "0.0008549690246582031\n",
      "0.0008633136749267578\n",
      "0.0008575916290283203\n",
      "0.0008573532104492188\n",
      "0.0008552074432373047\n",
      "0.0008573532104492188\n",
      "0.0008645057678222656\n",
      "0.0008554458618164062\n",
      "0.0008552074432373047\n",
      "0.0008592605590820312\n",
      "0.0008592605590820312\n",
      "0.0008630752563476562\n",
      "0.0008656978607177734\n",
      "0.0008628368377685547\n",
      "0.0008571147918701172\n",
      "0.0008702278137207031\n",
      "0.0008559226989746094\n",
      "0.0008721351623535156\n",
      "0.0008623600006103516\n",
      "0.0008645057678222656\n",
      "0.0008592605590820312\n",
      "0.0008587837219238281\n",
      "0.0008568763732910156\n",
      "0.0008542537689208984\n",
      "0.0008647441864013672\n",
      "0.0008580684661865234\n",
      "0.0008578300476074219\n",
      "0.0008575916290283203\n",
      "0.0008566379547119141\n",
      "0.0008711814880371094\n",
      "0.0008542537689208984\n",
      "0.0008563995361328125\n",
      "0.0008599758148193359\n",
      "0.0008649826049804688\n",
      "0.0008573532104492188\n",
      "0.0008587837219238281\n",
      "0.0008540153503417969\n",
      "0.000858306884765625\n",
      "0.0008645057678222656\n",
      "0.0008566379547119141\n",
      "0.0008573532104492188\n",
      "0.0008590221405029297\n",
      "0.0008618831634521484\n",
      "0.0008578300476074219\n",
      "0.0008571147918701172\n",
      "0.0008573532104492188\n",
      "0.0008556842803955078\n",
      "0.000865936279296875\n",
      "0.0008547306060791016\n",
      "0.0008585453033447266\n",
      "0.0008575916290283203\n",
      "0.0008585453033447266\n",
      "0.0008647441864013672\n",
      "0.0008554458618164062\n",
      "0.0008561611175537109\n",
      "0.0008578300476074219\n",
      "0.0008656978607177734\n",
      "0.0008587837219238281\n",
      "0.0008549690246582031\n",
      "0.0008571147918701172\n",
      "0.0008568763732910156\n",
      "0.0008604526519775391\n",
      "0.0008571147918701172\n",
      "0.0008587837219238281\n",
      "0.0008575916290283203\n",
      "0.0008692741394042969\n",
      "0.0008599758148193359\n",
      "0.0008580684661865234\n",
      "0.0008590221405029297\n",
      "0.0008561611175537109\n",
      "0.0008692741394042969\n",
      "0.0008594989776611328\n",
      "0.0008575916290283203\n",
      "0.0008573532104492188\n",
      "0.0008618831634521484\n",
      "0.0008606910705566406\n",
      "0.0008561611175537109\n",
      "0.0008587837219238281\n",
      "0.0008547306060791016\n",
      "0.0008771419525146484\n",
      "0.0008594989776611328\n",
      "0.0008561611175537109\n",
      "0.0008561611175537109\n",
      "0.0008573532104492188\n",
      "0.0008618831634521484\n",
      "0.0008587837219238281\n",
      "0.0008561611175537109\n",
      "0.0008568763732910156\n",
      "0.0008628368377685547\n",
      "0.002821207046508789\n",
      "0.001337289810180664\n",
      "0.0008912086486816406\n",
      "0.0008738040924072266\n",
      "0.0008676052093505859\n",
      "0.000865936279296875\n",
      "0.0008683204650878906\n",
      "0.0008611679077148438\n",
      "0.0008668899536132812\n",
      "0.0008628368377685547\n",
      "0.003614664077758789\n",
      "0.0008795261383056641\n",
      "0.0008687973022460938\n",
      "0.0008649826049804688\n",
      "0.0008645057678222656\n",
      "0.0008606910705566406\n",
      "0.0008697509765625\n",
      "0.0008606910705566406\n",
      "0.0008664131164550781\n",
      "0.0008604526519775391\n",
      "0.0008642673492431641\n",
      "0.0008745193481445312\n",
      "0.0008640289306640625\n",
      "0.0008606910705566406\n",
      "0.0008623600006103516\n",
      "0.0008692741394042969\n",
      "0.0008616447448730469\n",
      "0.0008599758148193359\n",
      "0.0008587837219238281\n",
      "0.0008623600006103516\n",
      "0.0008676052093505859\n",
      "0.0008652210235595703\n",
      "0.0008628368377685547\n",
      "0.0008606910705566406\n",
      "0.0008668899536132812\n",
      "0.0008642673492431641\n",
      "0.0008604526519775391\n",
      "0.0008640289306640625\n",
      "0.0008604526519775391\n",
      "0.0008752346038818359\n",
      "0.0008618831634521484\n",
      "0.0008633136749267578\n",
      "0.0008618831634521484\n",
      "0.0008714199066162109\n",
      "0.0008645057678222656\n",
      "0.0008609294891357422\n",
      "0.0008633136749267578\n",
      "0.0008623600006103516\n",
      "0.0008718967437744141\n",
      "0.0008618831634521484\n",
      "0.0008614063262939453\n",
      "0.0008599758148193359\n",
      "0.0008611679077148438\n",
      "0.0008692741394042969\n",
      "0.0008599758148193359\n",
      "0.0008635520935058594\n",
      "0.0008616447448730469\n",
      "0.0008685588836669922\n",
      "0.0008614063262939453\n",
      "0.0008602142333984375\n",
      "0.00086212158203125\n",
      "0.0008611679077148438\n",
      "0.0008606910705566406\n",
      "0.0009698867797851562\n",
      "0.0008649826049804688\n",
      "0.0008625984191894531\n",
      "0.0008676052093505859\n",
      "0.0008661746978759766\n",
      "0.0008640289306640625\n",
      "0.0008606910705566406\n",
      "0.0008611679077148438\n",
      "0.0009160041809082031\n",
      "0.0008671283721923828\n",
      "0.0009102821350097656\n",
      "0.0008878707885742188\n",
      "0.0009109973907470703\n",
      "0.0008771419525146484\n",
      "0.0008640289306640625\n",
      "0.0008599758148193359\n",
      "0.0008635520935058594\n",
      "0.0008978843688964844\n",
      "0.0008616447448730469\n",
      "0.0008575916290283203\n",
      "0.0008575916290283203\n",
      "0.0008640289306640625\n",
      "0.0008616447448730469\n",
      "0.0008606910705566406\n",
      "0.0008602142333984375\n",
      "0.000858306884765625\n",
      "0.0008671283721923828\n",
      "0.0008592605590820312\n",
      "0.0008571147918701172\n",
      "0.0008568763732910156\n",
      "0.0008649826049804688\n",
      "0.0008585453033447266\n",
      "0.0008618831634521484\n",
      "0.0008559226989746094\n",
      "0.0008580684661865234\n",
      "0.0008683204650878906\n",
      "0.0008742809295654297\n",
      "0.0008609294891357422\n",
      "0.0008590221405029297\n",
      "0.0008568763732910156\n",
      "0.0008649826049804688\n",
      "0.0008597373962402344\n",
      "0.0008578300476074219\n",
      "0.0008592605590820312\n",
      "0.0008697509765625\n",
      "0.0008580684661865234\n",
      "0.0008575916290283203\n",
      "0.0008578300476074219\n",
      "0.000858306884765625\n",
      "0.0008656978607177734\n",
      "0.0008592605590820312\n",
      "0.0008575916290283203\n",
      "0.0008566379547119141\n",
      "0.0008592605590820312\n",
      "0.0008585453033447266\n",
      "0.0008578300476074219\n",
      "0.0008568763732910156\n",
      "0.0008575916290283203\n",
      "0.0008649826049804688\n",
      "0.0008587837219238281\n",
      "0.0008580684661865234\n",
      "0.000858306884765625\n",
      "0.0008625984191894531\n",
      "0.0008616447448730469\n",
      "0.0009326934814453125\n",
      "0.0008614063262939453\n",
      "0.0008606910705566406\n",
      "0.003493070602416992\n",
      "0.0008754730224609375\n",
      "0.0008676052093505859\n",
      "0.0008630752563476562\n",
      "0.0008606910705566406\n",
      "0.0008604526519775391\n",
      "0.0008649826049804688\n",
      "0.0008587837219238281\n",
      "0.0008602142333984375\n",
      "0.0008847713470458984\n",
      "0.0008678436279296875\n",
      "0.0008676052093505859\n",
      "0.0008592605590820312\n",
      "0.0008609294891357422\n",
      "0.0008614063262939453\n",
      "0.0008642673492431641\n",
      "0.0008580684661865234\n",
      "0.0008561611175537109\n",
      "0.000858306884765625\n",
      "0.0008561611175537109\n",
      "0.0008640289306640625\n",
      "0.0008587837219238281\n",
      "0.0008616447448730469\n",
      "0.0008587837219238281\n",
      "0.0008652210235595703\n",
      "0.0008604526519775391\n",
      "0.0009260177612304688\n",
      "0.0008645057678222656\n",
      "0.0008585453033447266\n",
      "0.0008647441864013672\n",
      "0.0008571147918701172\n",
      "0.0008571147918701172\n",
      "0.0008561611175537109\n",
      "0.0008645057678222656\n",
      "0.0008623600006103516\n",
      "0.0008585453033447266\n",
      "0.0008609294891357422\n",
      "0.0008590221405029297\n",
      "0.0008633136749267578\n",
      "0.0008585453033447266\n",
      "0.0008575916290283203\n",
      "0.0008549690246582031\n",
      "0.0008542537689208984\n",
      "0.0008647441864013672\n",
      "0.0008573532104492188\n",
      "0.0008575916290283203\n",
      "0.0008580684661865234\n",
      "0.0008645057678222656\n",
      "0.0008635520935058594\n",
      "0.0008566379547119141\n",
      "0.0008604526519775391\n",
      "0.0008578300476074219\n",
      "0.0008671283721923828\n",
      "0.0008633136749267578\n",
      "0.0008556842803955078\n",
      "0.0008566379547119141\n",
      "0.0008683204650878906\n",
      "0.0008618831634521484\n",
      "0.0008592605590820312\n",
      "0.0008554458618164062\n",
      "0.0008602142333984375\n",
      "0.0008668899536132812\n",
      "0.0008573532104492188\n",
      "0.0008573532104492188\n",
      "0.0008559226989746094\n",
      "0.0008549690246582031\n",
      "0.0008652210235595703\n",
      "0.0008580684661865234\n",
      "0.0008561611175537109\n",
      "0.0008573532104492188\n",
      "0.0008649826049804688\n",
      "0.0008599758148193359\n",
      "0.0008575916290283203\n",
      "0.0008575916290283203\n",
      "0.0008559226989746094\n",
      "0.0008845329284667969\n",
      "0.0008642673492431641\n",
      "0.0008637905120849609\n",
      "0.000858306884765625\n",
      "0.0008645057678222656\n",
      "0.0008597373962402344\n",
      "0.0008594989776611328\n",
      "0.0008554458618164062\n",
      "0.0008597373962402344\n",
      "0.0008645057678222656\n",
      "0.0008597373962402344\n",
      "0.000858306884765625\n",
      "0.0008573532104492188\n",
      "0.0008566379547119141\n",
      "0.0008673667907714844\n",
      "0.0008578300476074219\n",
      "0.0008556842803955078\n",
      "0.0008580684661865234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001873016357421875\n",
      "0.0011818408966064453\n",
      "0.000865936279296875\n",
      "0.0008699893951416016\n",
      "0.0008614063262939453\n",
      "0.0008616447448730469\n",
      "0.0008599758148193359\n",
      "0.0008592605590820312\n",
      "0.0008695125579833984\n",
      "0.0008580684661865234\n",
      "0.0008592605590820312\n",
      "0.0008585453033447266\n",
      "0.0008645057678222656\n",
      "0.0008616447448730469\n",
      "0.0008578300476074219\n",
      "0.0008592605590820312\n",
      "0.0008578300476074219\n",
      "0.0008652210235595703\n",
      "0.0008587837219238281\n",
      "0.0008575916290283203\n",
      "0.0008580684661865234\n",
      "0.0008637905120849609\n",
      "0.0008625984191894531\n",
      "0.0008568763732910156\n",
      "0.0008566379547119141\n",
      "0.0008571147918701172\n",
      "0.003215312957763672\n",
      "0.0008685588836669922\n",
      "0.0008695125579833984\n",
      "0.0008599758148193359\n",
      "0.0008604526519775391\n",
      "0.0008602142333984375\n",
      "0.0008654594421386719\n",
      "0.0008633136749267578\n",
      "0.0008625984191894531\n",
      "0.0008599758148193359\n",
      "0.0008599758148193359\n",
      "0.0008652210235595703\n",
      "0.0008614063262939453\n",
      "0.0008580684661865234\n",
      "0.000858306884765625\n",
      "0.0008609294891357422\n",
      "0.0008690357208251953\n",
      "0.0008590221405029297\n",
      "0.000858306884765625\n",
      "0.0008616447448730469\n",
      "0.0008647441864013672\n",
      "0.00086212158203125\n",
      "0.00086212158203125\n",
      "0.0008666515350341797\n",
      "0.0008566379547119141\n",
      "0.0008666515350341797\n",
      "0.0008642673492431641\n",
      "0.0008702278137207031\n",
      "0.0008838176727294922\n",
      "0.0009355545043945312\n",
      "0.0008466243743896484\n",
      "0.0008571147918701172\n",
      "0.0008838176727294922\n",
      "0.0008616447448730469\n",
      "0.0008656978607177734\n",
      "0.0008614063262939453\n",
      "0.0008611679077148438\n",
      "0.0008585453033447266\n",
      "0.0008666515350341797\n",
      "0.00086212158203125\n",
      "0.0008585453033447266\n",
      "0.0008554458618164062\n",
      "0.0008580684661865234\n",
      "0.0008635520935058594\n",
      "0.000858306884765625\n",
      "0.0008590221405029297\n",
      "0.0008571147918701172\n",
      "0.0008561611175537109\n",
      "0.0008609294891357422\n",
      "0.0008563995361328125\n",
      "0.0008571147918701172\n",
      "0.0008580684661865234\n",
      "0.0008647441864013672\n",
      "0.0008571147918701172\n",
      "0.0008563995361328125\n",
      "0.0008599758148193359\n",
      "0.0008575916290283203\n",
      "0.0008637905120849609\n",
      "0.0008590221405029297\n",
      "0.0008609294891357422\n",
      "0.0008592605590820312\n",
      "0.0008666515350341797\n",
      "0.0008611679077148438\n",
      "0.0008571147918701172\n",
      "0.0008585453033447266\n",
      "0.000858306884765625\n",
      "0.0008661746978759766\n",
      "0.000858306884765625\n",
      "0.0008573532104492188\n",
      "0.0008587837219238281\n",
      "0.0008566379547119141\n",
      "0.0008661746978759766\n",
      "0.0008616447448730469\n",
      "0.0008580684661865234\n",
      "0.0008559226989746094\n",
      "0.0008654594421386719\n",
      "0.000858306884765625\n",
      "0.0008592605590820312\n",
      "0.0008585453033447266\n",
      "0.0008592605590820312\n",
      "0.0008637905120849609\n",
      "0.0008602142333984375\n",
      "0.0008573532104492188\n",
      "0.0008587837219238281\n",
      "0.0008630752563476562\n",
      "0.0008599758148193359\n",
      "0.0008568763732910156\n",
      "0.000858306884765625\n",
      "0.0008544921875\n",
      "0.0008654594421386719\n",
      "0.0008578300476074219\n",
      "0.0008568763732910156\n",
      "0.0008580684661865234\n",
      "0.00086212158203125\n",
      "0.0008633136749267578\n",
      "0.0008656978607177734\n",
      "0.0008568763732910156\n",
      "0.0008568763732910156\n",
      "0.0008673667907714844\n",
      "0.0008625984191894531\n",
      "0.0008585453033447266\n",
      "0.0008556842803955078\n",
      "0.0008587837219238281\n",
      "0.0008623600006103516\n",
      "0.0008611679077148438\n",
      "0.0008585453033447266\n",
      "0.000858306884765625\n",
      "0.0008616447448730469\n",
      "0.0008602142333984375\n",
      "0.0008587837219238281\n",
      "0.0008566379547119141\n",
      "0.000858306884765625\n",
      "0.0008647441864013672\n",
      "0.0008580684661865234\n",
      "0.0008580684661865234\n",
      "0.0008587837219238281\n",
      "0.0008611679077148438\n",
      "0.0008606910705566406\n",
      "0.0008559226989746094\n",
      "0.0008585453033447266\n",
      "0.0008547306060791016\n",
      "0.003201723098754883\n",
      "0.0008635520935058594\n",
      "0.0008647441864013672\n",
      "0.0008628368377685547\n",
      "0.0008606910705566406\n",
      "0.0008599758148193359\n",
      "0.000865936279296875\n",
      "0.0008640289306640625\n",
      "0.0008585453033447266\n",
      "0.0008597373962402344\n",
      "0.0008625984191894531\n",
      "0.0008664131164550781\n",
      "0.0008587837219238281\n",
      "0.0008578300476074219\n",
      "0.0008618831634521484\n",
      "0.0008578300476074219\n",
      "0.0008714199066162109\n",
      "0.0008563995361328125\n",
      "0.0008611679077148438\n",
      "0.0008594989776611328\n",
      "0.00086212158203125\n",
      "0.0008597373962402344\n",
      "0.0008594989776611328\n",
      "0.0008561611175537109\n",
      "0.0008580684661865234\n",
      "0.0008652210235595703\n",
      "0.0008592605590820312\n",
      "0.0008573532104492188\n",
      "0.0008568763732910156\n",
      "0.0008642673492431641\n",
      "0.0008585453033447266\n",
      "0.0008614063262939453\n",
      "0.0008580684661865234\n",
      "0.0008573532104492188\n",
      "0.0008649826049804688\n",
      "0.0008592605590820312\n",
      "0.0008592605590820312\n",
      "0.0008578300476074219\n",
      "0.0008580684661865234\n",
      "0.0008687973022460938\n",
      "0.0008590221405029297\n",
      "0.0008609294891357422\n",
      "0.0008635520935058594\n",
      "0.0008635520935058594\n",
      "0.000858306884765625\n",
      "0.0008559226989746094\n",
      "0.000858306884765625\n",
      "0.0008547306060791016\n",
      "0.0008685588836669922\n",
      "0.0008587837219238281\n",
      "0.000858306884765625\n",
      "0.0008609294891357422\n",
      "0.0008637905120849609\n",
      "0.0008609294891357422\n",
      "0.0008606910705566406\n",
      "0.0008566379547119141\n",
      "0.0008573532104492188\n",
      "0.0008640289306640625\n",
      "0.0008609294891357422\n",
      "0.0008568763732910156\n",
      "0.0008587837219238281\n",
      "0.0008590221405029297\n",
      "0.0008633136749267578\n",
      "0.0008590221405029297\n",
      "0.0008578300476074219\n",
      "0.0008561611175537109\n",
      "0.00086212158203125\n",
      "0.0008556842803955078\n",
      "0.0008592605590820312\n",
      "0.0008552074432373047\n",
      "0.0008575916290283203\n",
      "0.0008645057678222656\n",
      "0.0008599758148193359\n",
      "0.0008587837219238281\n",
      "0.0008592605590820312\n",
      "0.0008592605590820312\n",
      "0.0008580684661865234\n",
      "0.0008578300476074219\n",
      "0.0022025108337402344\n",
      "0.0013346672058105469\n",
      "0.0008947849273681641\n",
      "0.0008783340454101562\n",
      "0.0008766651153564453\n",
      "0.0008642673492431641\n",
      "0.0008602142333984375\n",
      "0.0008628368377685547\n",
      "0.0008633136749267578\n",
      "0.0008723735809326172\n",
      "0.0008609294891357422\n",
      "0.0008592605590820312\n",
      "0.0008592605590820312\n",
      "0.0008668899536132812\n",
      "0.0008623600006103516\n",
      "0.0008654594421386719\n",
      "0.0008642673492431641\n",
      "0.0008597373962402344\n",
      "0.0008730888366699219\n",
      "0.0008602142333984375\n",
      "0.000858306884765625\n",
      "0.0008561611175537109\n",
      "0.0008575916290283203\n",
      "0.0008702278137207031\n",
      "0.00086212158203125\n",
      "0.0008580684661865234\n",
      "0.000858306884765625\n",
      "0.000865936279296875\n",
      "0.0008604526519775391\n",
      "0.0008571147918701172\n",
      "0.0008571147918701172\n",
      "0.0008575916290283203\n",
      "0.0008642673492431641\n",
      "0.0008559226989746094\n",
      "0.0008578300476074219\n",
      "0.000858306884765625\n",
      "0.0008654594421386719\n",
      "0.0008587837219238281\n",
      "0.0008587837219238281\n",
      "0.0008592605590820312\n",
      "0.0008592605590820312\n",
      "0.0032143592834472656\n",
      "0.0008661746978759766\n",
      "0.0008697509765625\n",
      "0.0008614063262939453\n",
      "0.0008575916290283203\n",
      "0.0008599758148193359\n",
      "0.0008673667907714844\n",
      "0.0008630752563476562\n",
      "0.0008594989776611328\n",
      "0.0008614063262939453\n",
      "0.0008559226989746094\n",
      "0.0008668899536132812\n",
      "0.0008554458618164062\n",
      "0.0008602142333984375\n",
      "0.0008587837219238281\n",
      "0.0008597373962402344\n",
      "0.0008614063262939453\n",
      "0.0008604526519775391\n",
      "0.0008590221405029297\n",
      "0.0008594989776611328\n",
      "0.0008683204650878906\n",
      "0.0008575916290283203\n",
      "0.0008614063262939453\n",
      "0.0008594989776611328\n",
      "0.0008559226989746094\n",
      "0.0008656978607177734\n",
      "0.0008578300476074219\n",
      "0.0008568763732910156\n",
      "0.0008585453033447266\n",
      "0.0008695125579833984\n",
      "0.0008602142333984375\n",
      "0.0008606910705566406\n",
      "0.0008614063262939453\n",
      "0.0008611679077148438\n",
      "0.0008676052093505859\n",
      "0.0008578300476074219\n",
      "0.0008587837219238281\n",
      "0.0008552074432373047\n",
      "0.0008666515350341797\n",
      "0.0008606910705566406\n",
      "0.0008571147918701172\n",
      "0.0008575916290283203\n",
      "0.0008585453033447266\n",
      "0.0008637905120849609\n",
      "0.0008578300476074219\n",
      "0.0008590221405029297\n",
      "0.0008585453033447266\n",
      "0.0008604526519775391\n",
      "0.0008645057678222656\n",
      "0.0008590221405029297\n",
      "0.0008566379547119141\n",
      "0.0008573532104492188\n",
      "0.0008654594421386719\n",
      "0.0008575916290283203\n",
      "0.0008604526519775391\n",
      "0.0008580684661865234\n",
      "0.000858306884765625\n",
      "0.0008673667907714844\n",
      "0.0008585453033447266\n",
      "0.0008618831634521484\n",
      "0.0008585453033447266\n",
      "0.0008637905120849609\n",
      "0.0008604526519775391\n",
      "0.0008585453033447266\n",
      "0.0008571147918701172\n",
      "0.0008609294891357422\n",
      "0.0008673667907714844\n",
      "0.0008585453033447266\n",
      "0.0008590221405029297\n",
      "0.0008571147918701172\n",
      "0.0008842945098876953\n",
      "0.0008800029754638672\n",
      "0.0008699893951416016\n",
      "0.0008745193481445312\n",
      "0.0009496212005615234\n",
      "0.0008716583251953125\n",
      "0.0008599758148193359\n",
      "0.0008749961853027344\n",
      "0.0008780956268310547\n",
      "0.0008854866027832031\n",
      "0.0008633136749267578\n",
      "0.0008590221405029297\n",
      "0.0008592605590820312\n",
      "0.0008614063262939453\n",
      "0.0008592605590820312\n",
      "0.0008568763732910156\n",
      "0.0008575916290283203\n",
      "0.0008554458618164062\n",
      "0.0008585453033447266\n",
      "0.0008618831634521484\n",
      "0.0008549690246582031\n",
      "0.0008547306060791016\n",
      "0.0008542537689208984\n",
      "0.0008635520935058594\n",
      "0.0008571147918701172\n",
      "0.0008563995361328125\n",
      "0.0008556842803955078\n",
      "0.0008573532104492188\n",
      "0.0008623600006103516\n",
      "0.0008573532104492188\n",
      "0.0008563995361328125\n",
      "0.0008566379547119141\n",
      "0.0008676052093505859\n",
      "0.0008580684661865234\n",
      "0.0008580684661865234\n",
      "0.0008573532104492188\n",
      "0.0008592605590820312\n",
      "0.0008673667907714844\n",
      "0.0008561611175537109\n",
      "0.000858306884765625\n",
      "0.0008559226989746094\n",
      "0.0008571147918701172\n",
      "0.0008625984191894531\n",
      "0.0008561611175537109\n",
      "0.0008563995361328125\n",
      "0.0008573532104492188\n",
      "0.0008630752563476562\n",
      "0.0008602142333984375\n",
      "0.0008568763732910156\n",
      "0.0008573532104492188\n",
      "0.0008599758148193359\n",
      "0.0008642673492431641\n",
      "0.0008575916290283203\n",
      "0.0008554458618164062\n",
      "0.0008585453033447266\n",
      "0.0031752586364746094\n",
      "0.0008764266967773438\n",
      "0.0008702278137207031\n",
      "0.0008623600006103516\n",
      "0.0008594989776611328\n",
      "0.0008609294891357422\n",
      "0.0008578300476074219\n",
      "0.0008652210235595703\n",
      "0.0008604526519775391\n",
      "0.0008611679077148438\n",
      "0.0008592605590820312\n",
      "0.0008652210235595703\n",
      "0.0008614063262939453\n",
      "0.0008587837219238281\n",
      "0.0008575916290283203\n",
      "0.0008587837219238281\n",
      "0.0008633136749267578\n",
      "0.0008597373962402344\n",
      "0.0008609294891357422\n",
      "0.0008575916290283203\n",
      "0.0008549690246582031\n",
      "0.0008647441864013672\n",
      "0.0008544921875\n",
      "0.0008571147918701172\n",
      "0.0008578300476074219\n",
      "0.0008656978607177734\n",
      "0.0008587837219238281\n",
      "0.0008561611175537109\n",
      "0.0008544921875\n",
      "0.000858306884765625\n",
      "0.0008678436279296875\n",
      "0.0008571147918701172\n",
      "0.0008590221405029297\n",
      "0.0008580684661865234\n",
      "0.0008652210235595703\n",
      "0.0008599758148193359\n",
      "0.0008604526519775391\n",
      "0.0008599758148193359\n",
      "0.0008561611175537109\n",
      "0.0008640289306640625\n",
      "0.0008590221405029297\n",
      "0.0008611679077148438\n",
      "0.0008590221405029297\n",
      "0.0008573532104492188\n",
      "0.000865936279296875\n",
      "0.0008585453033447266\n",
      "0.0008575916290283203\n",
      "0.0008575916290283203\n",
      "0.0008821487426757812\n",
      "0.0008635520935058594\n",
      "0.0008554458618164062\n",
      "0.0008594989776611328\n",
      "0.0008611679077148438\n",
      "0.0008661746978759766\n",
      "0.0008642673492431641\n",
      "0.0008599758148193359\n",
      "0.0008578300476074219\n",
      "0.000881195068359375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0018763542175292969\n",
      "0.0013370513916015625\n",
      "0.0009095668792724609\n",
      "0.0008795261383056641\n",
      "0.0008714199066162109\n",
      "0.0008625984191894531\n",
      "0.0008652210235595703\n",
      "0.0008909702301025391\n",
      "0.0008649826049804688\n",
      "0.0008587837219238281\n",
      "0.0008594989776611328\n",
      "0.0008716583251953125\n",
      "0.0008919239044189453\n",
      "0.0008614063262939453\n",
      "0.0008614063262939453\n",
      "0.0008549690246582031\n",
      "0.0008668899536132812\n",
      "0.0008568763732910156\n",
      "0.0008592605590820312\n",
      "0.0008568763732910156\n",
      "0.0008654594421386719\n",
      "0.0008602142333984375\n",
      "0.0008568763732910156\n",
      "0.0008597373962402344\n",
      "0.0008573532104492188\n",
      "0.00086212158203125\n",
      "0.000858306884765625\n",
      "0.0008592605590820312\n",
      "0.0008568763732910156\n",
      "0.0008597373962402344\n",
      "0.0008604526519775391\n",
      "0.000858306884765625\n",
      "0.0008592605590820312\n",
      "0.0008563995361328125\n",
      "0.0008630752563476562\n",
      "0.0008578300476074219\n",
      "0.0008573532104492188\n",
      "0.0008611679077148438\n",
      "0.0008580684661865234\n",
      "0.0008647441864013672\n",
      "0.0008609294891357422\n",
      "0.0008571147918701172\n",
      "0.0008571147918701172\n",
      "0.0008640289306640625\n",
      "0.0008592605590820312\n",
      "0.0008602142333984375\n",
      "0.0008616447448730469\n",
      "0.0008573532104492188\n",
      "0.0008656978607177734\n",
      "0.0008597373962402344\n",
      "0.0008575916290283203\n",
      "0.0008592605590820312\n",
      "0.0008580684661865234\n",
      "0.0008573532104492188\n",
      "0.0008587837219238281\n",
      "0.0008602142333984375\n",
      "0.0008575916290283203\n",
      "0.0008563995361328125\n",
      "0.0008561611175537109\n",
      "0.0008575916290283203\n",
      "0.0008571147918701172\n",
      "0.0008537769317626953\n",
      "0.003242969512939453\n",
      "0.0008795261383056641\n",
      "0.0008704662322998047\n",
      "0.0008614063262939453\n",
      "0.0008647441864013672\n",
      "0.0008623600006103516\n",
      "0.0008661746978759766\n",
      "0.0008614063262939453\n",
      "0.0008630752563476562\n",
      "0.0008592605590820312\n",
      "0.0008594989776611328\n",
      "0.0008661746978759766\n",
      "0.00086212158203125\n",
      "0.0008599758148193359\n",
      "0.0008590221405029297\n",
      "0.0008676052093505859\n",
      "0.0008618831634521484\n",
      "0.0008611679077148438\n",
      "0.0008609294891357422\n",
      "0.0008602142333984375\n",
      "0.0008678436279296875\n",
      "0.0008585453033447266\n",
      "0.0008594989776611328\n",
      "0.0008585453033447266\n",
      "0.0008618831634521484\n",
      "0.0008590221405029297\n",
      "0.0008606910705566406\n",
      "0.0008604526519775391\n",
      "0.0008578300476074219\n",
      "0.0008661746978759766\n",
      "0.0008592605590820312\n",
      "0.0008611679077148438\n",
      "0.0008580684661865234\n",
      "0.0008616447448730469\n",
      "0.0008666515350341797\n",
      "0.0008585453033447266\n",
      "0.0008573532104492188\n",
      "0.0009469985961914062\n",
      "0.0008692741394042969\n",
      "0.0008606910705566406\n",
      "0.0008592605590820312\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "aa = result[0]\n",
    "bb = result[1]\n",
    "# c : 0 x1_idx : 0, y1_idx : 2040, x2_idx : 4080 y2_idx : 6120\n",
    "# c : 1 x1_idx : 8160, y1_idx : 10200, x2_idx : 12240 y2_idx : 14280\n",
    "# c : 2 x1_idx : 16320, y1_idx : 18360, x2_idx : 20400 y2_idx : 22440\n",
    "\n",
    "ori_w = 640\n",
    "ori_h = 426\n",
    "def postprocess(aa,bb):\n",
    "\n",
    "    a = torch.tensor(aa)\n",
    "    b = torch.tensor(bb)\n",
    "    output_list = []\n",
    "    \n",
    "    x1 = torch.cat([a[:2040],a[8160:10200],a[16320:18360]],dim=0)\n",
    "    y1 = torch.cat([a[2040:4080],a[10200:12240],a[18360:20400]],dim=0)\n",
    "    x2 = torch.cat([a[4080:6120],a[12240:14280],a[20400:22440]],dim=0)\n",
    "    y2 = torch.cat([a[6120:8160],a[14280:16320],a[22440:]],dim=0)\n",
    "\n",
    "    tensor_x1 = (x1 - norm_data_x) * -35\n",
    "    tensor_y1 = (y1 - norm_data_y) * -35\n",
    "    tensor_x2 = (x2 + norm_data_x) * 35\n",
    "    tensor_y2 = (y2 + norm_data_y) * 35\n",
    "\n",
    "    all_tensor = torch.stack([tensor_x1,tensor_y1,tensor_x2,tensor_y2],dim=1)\n",
    "    score_tensor = result[1] >=0.7\n",
    "\n",
    "    box = all_tensor[score_tensor]\n",
    "    score = b[score_tensor]\n",
    "    label = label_tensor[score_tensor]\n",
    "\n",
    "    nms_out_index = torchvision.ops.batched_nms(\n",
    "        box,\n",
    "        score,\n",
    "        label,\n",
    "        0.5,\n",
    "    )\n",
    "    result_box = box[nms_out_index].detach().cpu().numpy()\n",
    "    result_score = score[nms_out_index].detach().cpu().numpy()\n",
    "\n",
    "    for idx, bbox in enumerate(result_box):\n",
    "        if isinstance(bbox, type(None)):\n",
    "            output_list.append(None)\n",
    "            continue\n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "        x1 = int((x1 * ori_w)/input_w)\n",
    "        y1 = int((y1 * ori_h)/input_h)\n",
    "        x2 = int((x2 * ori_w)/input_w)\n",
    "        y2 = int((y2 * ori_h)/input_h)\n",
    "        score = str(result_score[idx])\n",
    "        out = {\"bbox\":[x1, y1, x2, y2], \"score\":score, \"label\":label}\n",
    "        output_list.append(out)\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "for i in range(1000) :\n",
    "    s1 = time.time()\n",
    "    postprocess(aa,bb)\n",
    "    print((time.time()-s1))\n",
    "\n",
    "\n",
    "\n",
    "#     x1 = torch.cat([a[:2040],a[8160:10200],a[16320:18360]],dim=0)\n",
    "#     y1 = torch.cat([a[2040:4080],a[10200:12240],a[18360:20400]],dim=0)\n",
    "#     x2 = torch.cat([a[4080:6120],a[12240:14280],a[20400:22440]],dim=0)\n",
    "#     y2 = torch.cat([a[6120:8160],a[14280:16320],a[22440:]],dim=0)\n",
    "\n",
    "#     tensor_x1 = (x1 - norm_data_x) * -35\n",
    "#     tensor_y1 = (y1 - norm_data_y) * -35\n",
    "#     tensor_x2 = (x2 + norm_data_x) * 35\n",
    "#     tensor_y2 = (y2 + norm_data_y) * 35\n",
    "\n",
    "#     all_tensor = torch.stack([tensor_x1,tensor_y1,tensor_x2,tensor_y2,label_tensor],dim=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05e33db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
